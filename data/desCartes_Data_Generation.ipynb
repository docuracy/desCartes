{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkOjlqSD4fITPI0IjNibN6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docuracy/desCartes/blob/main/data/desCartes_Data_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgh1zXwuFOhB"
      },
      "source": [
        "# Road Vector Extraction Project Framework\n",
        "\n",
        "## 1. Initial Data Preparation\n",
        "\n",
        "### Annotate fully orthogonal rectangular areas using [QGIS](https://www.qgis.org/en/site/)\n",
        "\n",
        "* Set project CRS to EPSG:3857.\n",
        "* Configure project snapping settings.\n",
        "* Select scale for maximum magnification (e.g. 1:3390 for zoom level 16). The Magnifier control can be used to facilitate annotation.\n",
        "* Add template GeoPackage from [here](https://drive.google.com/file/d/1-61VwwLWoeOGk4lsfRx620aFJwrXkCE1/view?usp=sharing).\n",
        "* Enable editing in the `regions` layer, and using the \"Add Rectangle from Extent\" tool (Shape Digitizing Toolbar) draw the region(s) that you wish to annotate.\n",
        "* Open the labels-regions attribute table and add the XYZ URL for the basemap which is to be annotated (see [National Library of Scotland](https://maps.nls.uk/guides/georeferencing/qgis/) for examples). Indicate in the `annotated` column whether or not the region is to be annotated (otherwise it will be used for testing).\n",
        "* Fully annotate every road and path within each `annotated` region, paying particular attention to the location of junctions. Lines that meet or cross the boundary of a region should be extended a little way beyond that boundary. Use the following codes to distinguish between different types of road or path, dividing them into sections if necessary (descriptions below apply to OS 6\" maps):\n",
        "** 1: Main road (parallel thick and thin lines)\n",
        "** 2: Minor road (parallel thin lines)\n",
        "** 3: Semi-enclosed path (parallel solid and dashed lines)\n",
        "** 4: Unenclosed path (parallel dashed lines)\n",
        "\n",
        "## 2. Train [Ilastik](https://www.ilastik.org/) Pixel Classifier\n",
        "\n",
        "### Not necessary if you intend to work with 6\" Ordnance Survey maps\n",
        "\n",
        "* Use the `Generate & Augment Training Data` cell below to fetch map tiles and to create from them geotiffs covering the rectangular areas covered by your annotations.\n",
        "* Train Ilastik to classify background, roads, and other features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pagxqe-djTb-"
      },
      "outputs": [],
      "source": [
        "#@title Authenticate GCS, mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!gcloud auth application-default login\n",
        "!gcloud config set project descartes-404713"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMuhev-dZtw_",
        "outputId": "459aeb15-d637-4c4b-b3ed-f69ff684bb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.1 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n"
          ]
        }
      ],
      "source": [
        "#@title Upgrade TensorFlow\n",
        "\n",
        "!pip install --upgrade tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uQNXRHBq9-NY"
      },
      "outputs": [],
      "source": [
        "#@title Initialise directories and global variables\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Directory containing scripts such as 'map_from_tiles'\n",
        "scripts_directory = '/content/drive/MyDrive/Colab Notebooks/scripts'\n",
        "sys.path.append(scripts_directory)\n",
        "\n",
        "# Directories used by 'map_from_tiles'\n",
        "temp_directory = f\"{scripts_directory}/temp\"\n",
        "cache_directory = f\"{scripts_directory}/data/cache\"\n",
        "\n",
        "training_data_directory = '/content/drive/MyDrive/desCartes/training_data/'\n",
        "map_directory = f\"{training_data_directory}maps/\"\n",
        "map_classified_s1_directory = f\"{map_directory}classified_s1/\"\n",
        "map_one_inch_directory = f\"{map_directory}one_inch/\"\n",
        "map_osm_directory = f\"{map_directory}osm/\"\n",
        "map_dem_directory = f\"{map_directory}dem/\"\n",
        "map_elevation_directory = f\"{map_dem_directory}elevation/\"\n",
        "map_slope_directory = f\"{map_dem_directory}slope/\"\n",
        "map_augmented_s1_directory = f\"{map_directory}augmented_s1/\"\n",
        "map_binary_directory = f\"{map_directory}binary/\"\n",
        "map_skeleton_directory = f\"{map_directory}skeleton/\"\n",
        "map_output_directory = f\"{map_directory}output/\"\n",
        "map_mask_directory = f\"{map_output_directory}masks/\"\n",
        "map_overlay_directory = f\"{map_output_directory}overlays/\"\n",
        "map_geotiff_directory = f\"{map_output_directory}geotiffs/\"\n",
        "labels_directory = f\"{map_directory}labels/\"\n",
        "labels_raster_directory = f\"{labels_directory}raster/\"\n",
        "labels_overlay_directory = f\"{labels_directory}overlay/\"\n",
        "\n",
        "tile_directory = '/content/tiles/'\n",
        "tile_size = 256 # (px)\n",
        "min_overlap = 16 # Minimum tile overlap (px)\n",
        "\n",
        "# GeoPackage containing map annotations created in QGIS\n",
        "geopackage_path = '/content/drive/MyDrive/desCartes/templates/labels.gpkg'\n",
        "linestring_buffer = 3 # (px) Use False for no buffer\n",
        "\n",
        "maptiler_key = 'U2vLM8EbXurAd3Gq6C45'\n",
        "\n",
        "# UK Great Britain, Ordnance Survey six-inch to the mile (1:10,560), 1888-1913 https://cloud.maptiler.com/tiles/uk-osgb10k1888/\n",
        "basemap_url = 'https://api.maptiler.com/tiles/uk-osgb10k1888/{z}/{x}/{y}.jpg' + f'?key={maptiler_key}'\n",
        "\n",
        "# UK Great Britain, Ordnance Survey one-inch to the mile (1:63,360), 1888-1913 https://cloud.maptiler.com/tiles/uk-osgb63k1885/\n",
        "basemap_url_one_inch = 'https://api.maptiler.com/tiles/uk-osgb63k1885/{z}/{x}/{y}.png' + f'?key={maptiler_key}'\n",
        "\n",
        "# DEM Tiles - see https://documentation.maptiler.com/hc/en-us/articles/4405444055313-RGB-Terrain-by-MapTiler\n",
        "dem_tilesource = 'https://api.maptiler.com/tiles/terrain-rgb-v2/{z}/{x}/{y}.webp' + f'?key={maptiler_key}'\n",
        "dem_max_zoom = 14\n",
        "\n",
        "# Ilastik model used for Stage 1 pixel classification\n",
        "ilastik_project_file = \"/content/drive/MyDrive/desCartes/ilastik/preprocess.ilp\"\n",
        "ilastik_executable = './ilastik-1.4.0-Linux/run_ilastik.sh'\n",
        "\n",
        "# Directory for saving trained CNN models\n",
        "model_directory = \"/content/drive/MyDrive/desCartes/models\"\n",
        "\n",
        "label_strings_file = os.path.join(model_directory, 'label_strings.txt')\n",
        "class_weights_file = os.path.join(model_directory, 'class_weights.json')\n",
        "num_classes = 5 # Allows for fill (zero) and road classes 1 to 4 (determined by QGIS labelling)\n",
        "\n",
        "# Google Cloud Services\n",
        "gcs_key_path = '/content/drive/MyDrive/desCartes/descartes-404713-cccf7c3921aa.json'\n",
        "gcs_project_id = 'descartes-404713'\n",
        "gcs_bucket_name = 'descartes'\n",
        "gcs_data_directory = \"training_data\"\n",
        "\n",
        "# Set the split ratios and batch size for training data\n",
        "TFRecord_batch_size = 16\n",
        "train_ratio = 0.85\n",
        "eval_ratio = 0.15\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "\n",
        "# Inference: Color mappings for classes\n",
        "class_colors = {\n",
        "    0: (0, 0, 0, 0),  # Transparent (background)\n",
        "    1: (178,24,43,180),  # Red\n",
        "    2: (239,138,98,180),  # Orange\n",
        "    3: (84,39,136,180),  # Purple\n",
        "    4: (153,142,195,180),  # Lilac\n",
        "}\n",
        "\n",
        "# Create directories if they do not exist\n",
        "directories_to_create = [\n",
        "    temp_directory,\n",
        "    cache_directory,\n",
        "    training_data_directory,\n",
        "    map_directory,\n",
        "    map_one_inch_directory,\n",
        "    map_osm_directory,\n",
        "    map_dem_directory,\n",
        "    map_elevation_directory,\n",
        "    map_slope_directory,\n",
        "    map_classified_s1_directory,\n",
        "    map_augmented_s1_directory,\n",
        "    map_binary_directory,\n",
        "    map_skeleton_directory,\n",
        "    map_output_directory,\n",
        "    map_mask_directory,\n",
        "    map_overlay_directory,\n",
        "    map_geotiff_directory,\n",
        "    labels_directory,\n",
        "    labels_raster_directory,\n",
        "    labels_overlay_directory,\n",
        "    model_directory,\n",
        "]\n",
        "\n",
        "for directory in directories_to_create:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "def install_ilastik(): # Install only if/when needed\n",
        "    subprocess.run(['wget', 'https://files.ilastik.org/ilastik-1.4.0-Linux.tar.bz2'])\n",
        "    subprocess.run(['tar', 'xjf', 'ilastik-1.4.0-Linux.tar.bz2'])\n",
        "    subprocess.run(['rm', './ilastik-1.4.0-Linux.tar.bz2'])\n",
        "    sys.path.append('/content/ilastik-1.4.0-Linux/lib/python3.7/site-packages')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4gdF6DI1Ah6y"
      },
      "outputs": [],
      "source": [
        "#@title Load tile-fetching code\n",
        "\n",
        "'''\n",
        "map_from_tiles.py\n",
        "\n",
        "@author: Stephen Gadd, Docuracy Ltd, UK\n",
        "Adapted from https://github.com/jimutt/tiles-to-tiff\n",
        "\n",
        "This script is used to create a georeferenced map from a tile source. It uses\n",
        "the GDAL library to fetch, georeference and merge tiles of an image. The script\n",
        "takes in the tile source, output directory, map name, bounding box and\n",
        "zoom level as input. The bounding box is used to calculate the range of x and\n",
        "y coordinates of the tiles that need to be fetched. Once all the tiles are\n",
        "fetched, they are georeferenced and merged to create a single map file.\n",
        "\n",
        "'''\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import glob\n",
        "import shutil\n",
        "from osgeo import gdal\n",
        "import pyproj as proj\n",
        "import hashlib\n",
        "import base64\n",
        "from math import log, tan, radians, cos, pi, floor, degrees, atan, sinh\n",
        "\n",
        "gdal_options = {\n",
        "    'jpg': {'format': 'JPEG', 'creationOptions': ['PIXELTYPE=U8', 'JPEG_QUALITY=100', 'JPEG_SUBSAMPLE=0', 'PROGRESSIVE=NO']},\n",
        "    'webp': {'format': 'WEBP', 'creationOptions': ['RESAMPLING=NEAREST','QUALITY=100']},\n",
        "    'png': {'format': 'PNG', 'creationOptions': ['COMPRESS=DEFLATE', 'ZLEVEL=9']},\n",
        "    'tif': {'format': 'GTiff', 'creationOptions': None}\n",
        "    }\n",
        "\n",
        "def sec(x):\n",
        "    return(1/cos(x))\n",
        "\n",
        "\n",
        "def latlon_to_xyz(lat, lon, z):\n",
        "    tile_count = pow(2, z)\n",
        "    x = (lon + 180) / 360\n",
        "    y = (1 - log(tan(radians(lat)) + sec(radians(lat))) / pi) / 2\n",
        "    return(tile_count*x, tile_count*y)\n",
        "\n",
        "\n",
        "def bbox_to_xyz(lon_min, lon_max, lat_min, lat_max, z):\n",
        "    x_min, y_max = latlon_to_xyz(lat_min, lon_min, z)\n",
        "    x_max, y_min = latlon_to_xyz(lat_max, lon_max, z)\n",
        "    return(floor(x_min), floor(x_max),\n",
        "           floor(y_min), floor(y_max))\n",
        "\n",
        "\n",
        "def mercatorToLat(mercatorY):\n",
        "    return(degrees(atan(sinh(mercatorY))))\n",
        "\n",
        "\n",
        "def y_to_lat_edges(y, z):\n",
        "    tile_count = pow(2, z)\n",
        "    unit = 1 / tile_count\n",
        "    relative_y1 = y * unit\n",
        "    relative_y2 = relative_y1 + unit\n",
        "    lat1 = mercatorToLat(pi * (1 - 2 * relative_y1))\n",
        "    lat2 = mercatorToLat(pi * (1 - 2 * relative_y2))\n",
        "    return(lat1, lat2)\n",
        "\n",
        "\n",
        "def x_to_lon_edges(x, z):\n",
        "    tile_count = pow(2, z)\n",
        "    unit = 360 / tile_count\n",
        "    lon1 = -180 + x * unit\n",
        "    lon2 = lon1 + unit\n",
        "    return(lon1, lon2)\n",
        "\n",
        "\n",
        "def tile_edges(x, y, z):\n",
        "    lat1, lat2 = y_to_lat_edges(y, z)\n",
        "    lon1, lon2 = x_to_lon_edges(x, z)\n",
        "    return[lon1, lat1, lon2, lat2]\n",
        "\n",
        "\n",
        "def fetch_tile(x, y, z, tile_source, cache_dir, temp_dir, filetype):\n",
        "\n",
        "    cache_path = f'{cache_dir}/{x}_{y}_{z}.{filetype}'\n",
        "    if os.path.exists(cache_path):\n",
        "        shutil.copy(cache_path, temp_dir)\n",
        "        return cache_path\n",
        "\n",
        "    url = tile_source.replace(\n",
        "        \"{x}\", str(x)).replace(\n",
        "        \"{y}\", str(y)).replace(\n",
        "        \"{z}\", str(z)).replace(\n",
        "        \"%7Bx%7D\", str(x)).replace(\n",
        "        \"%7By%7D\", str(y)).replace(\n",
        "        \"%7Bz%7D\", str(z))\n",
        "\n",
        "    if not tile_source.startswith(\"http\"):\n",
        "        return url.replace(\"file:///\", \"\")\n",
        "\n",
        "    path = f'{temp_dir}/{x}_{y}_{z}.{filetype}'\n",
        "\n",
        "    req = urllib.request.Request(\n",
        "        url,\n",
        "        data=None,\n",
        "        headers={\n",
        "            'User-Agent': 'desCartes (+https://github.com/docuracy/desCartes)'\n",
        "        }\n",
        "    )\n",
        "    g = urllib.request.urlopen(req)\n",
        "\n",
        "    if filetype == 'webp': # Convert to png to avoid band distortion at georeferencing stage\n",
        "        webp_image = Image.open(BytesIO(g.read()))\n",
        "        png_image_path = path.replace(\".webp\", \".png\")\n",
        "        webp_image.save(png_image_path, format=\"PNG\")\n",
        "        return png_image_path\n",
        "\n",
        "    elif filetype == 'png': # Remove alpha channel to avoid distortion at mosaicing stage\n",
        "        png_image = Image.open(BytesIO(g.read()))\n",
        "        png_image.convert('RGB').save(path, format=\"PNG\")\n",
        "        return path\n",
        "\n",
        "    else:\n",
        "        with open(path, 'b+w') as f:\n",
        "            f.write(g.read())\n",
        "        return path\n",
        "\n",
        "def merge_tiles(input_pattern, output_path, extent, crs, temp_dir, filetype):\n",
        "\n",
        "    input_files = glob.glob(input_pattern)\n",
        "    if not input_files:\n",
        "        print(f\"No files found matching pattern: {input_pattern}\")\n",
        "        return\n",
        "\n",
        "    print(gdal_options[filetype])\n",
        "\n",
        "    vrt_path = os.path.join(temp_dir, \"tiles.vrt\")\n",
        "    gdal.BuildVRT(vrt_path, input_files, addAlpha=False)\n",
        "    print(f'Projecting {extent} to {crs}')\n",
        "    gdal.Translate(\n",
        "        output_path,\n",
        "        vrt_path,\n",
        "        outputSRS=crs,\n",
        "        projWin=[extent[0], extent[3], extent[2], extent[1]],\n",
        "        format=gdal_options[filetype]['format'],\n",
        "        creationOptions=gdal_options[filetype]['creationOptions'],\n",
        "        # resampleAlg='cubic'\n",
        "    )\n",
        "\n",
        "def georeference_raster_tile(x, y, z, path, crs, temp_dir, filetype, tilesize):\n",
        "    bounds = tile_edges(x, y, z)\n",
        "\n",
        "    # Create the projection transformer and transform from EPSG:4326\n",
        "    transformer = proj.Transformer.from_crs(\"EPSG:4326\", crs, always_xy=True)\n",
        "    bounds[0],bounds[1] = transformer.transform(bounds[0], bounds[1])\n",
        "    bounds[2],bounds[3] = transformer.transform(bounds[2], bounds[3])\n",
        "\n",
        "    # Save the original tile to a temporary file\n",
        "    original_tile_path = os.path.join(temp_dir, f'original_{x}_{y}_{z}.{filetype}')\n",
        "    shutil.copy(path, original_tile_path)\n",
        "\n",
        "    gdal.Translate(os.path.join(temp_dir, f'{temp_dir}/{x}_{y}_{z}.{filetype}'),\n",
        "           original_tile_path,\n",
        "           outputSRS=crs,\n",
        "           outputBounds=bounds,\n",
        "           width=tilesize['x'],\n",
        "           height=tilesize['y'],\n",
        "           format=gdal_options[filetype]['format'],\n",
        "           creationOptions=gdal_options[filetype]['creationOptions'],\n",
        "           )\n",
        "\n",
        "    os.remove(original_tile_path)\n",
        "\n",
        "def create_map(tile_source, output_dir, map_name, bounding_box, zoom, crs, temp_dir, cache_dir_root, filetype='jpg'):\n",
        "\n",
        "    filetype = tile_source.split('.')[-1].split('?')[0]\n",
        "\n",
        "    bounding_box_original = bounding_box\n",
        "\n",
        "    if not crs == 'EPSG:4326':\n",
        "\n",
        "        # Create the projection transformer to EPSG:4326\n",
        "        transformer = proj.Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n",
        "\n",
        "        # Extract the coordinates of the extent\n",
        "        xminOld, yminOld, xmaxOld, ymaxOld = bounding_box\n",
        "\n",
        "        # Use the transformer to convert the extent\n",
        "        xmin4326, ymin4326 = transformer.transform(xminOld, yminOld)\n",
        "        xmax4326, ymax4326 = transformer.transform(xmaxOld, ymaxOld)\n",
        "\n",
        "        bounding_box = (xmin4326, ymin4326, xmax4326, ymax4326)\n",
        "\n",
        "    # Print the extent\n",
        "    print(f\"Extent of {map_name}: {bounding_box}\")\n",
        "\n",
        "    lon_min, lat_min, lon_max, lat_max = bounding_box\n",
        "\n",
        "    # Create a cache directory name\n",
        "    hash_obj = hashlib.sha256(tile_source.encode())\n",
        "    hash_bytes = hash_obj.digest()\n",
        "    hash_b64 = base64.urlsafe_b64encode(hash_bytes).decode()\n",
        "    cache_dir = os.path.join(cache_dir_root, hash_b64)\n",
        "\n",
        "    # Script start:\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    x_min, x_max, y_min, y_max = bbox_to_xyz(\n",
        "        lon_min, lon_max, lat_min, lat_max, zoom)\n",
        "\n",
        "    total_tiles = (x_max - x_min + 1) * (y_max - y_min + 1)\n",
        "    counter = 0\n",
        "    tilesize = None\n",
        "    print(f\"Fetching & georeferencing {total_tiles} tiles...\")\n",
        "\n",
        "    for x in range(x_min, x_max + 1):\n",
        "        for y in range(y_min, y_max + 1):\n",
        "            counter += 1\n",
        "            try:\n",
        "                tile_path = fetch_tile(x, y, zoom, tile_source, cache_dir, temp_dir, filetype)\n",
        "                if filetype == 'webp': # (Converted by fetch_tile)\n",
        "                    filetype = 'png'\n",
        "                percent_done = counter / total_tiles * 100\n",
        "                print(f\"{percent_done:.1f}% : {x},{y} {'found in cache.' if cache_dir in tile_path else 'fetched from tileserver.'}\", end='\\r')\n",
        "                if tilesize is None:\n",
        "                    print(f'Fetching tile size of {tile_path}...')\n",
        "                    ds = gdal.Open(tile_path)\n",
        "                    tilesize = {'x': ds.RasterXSize, 'y': ds.RasterYSize}\n",
        "                    ds = None\n",
        "                    print(f'... {tilesize}')\n",
        "                georeference_raster_tile(x, y, zoom, tile_path, crs, temp_dir, filetype, tilesize)\n",
        "            except OSError:\n",
        "                print(f\"Error, failed to get {x},{y}\")\n",
        "                pass\n",
        "\n",
        "    if tilesize is None:\n",
        "        print(\"Failed to fetch any tiles for this extent.\")\n",
        "        filename = None\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(\"Resolving and georeferencing of raster tiles complete.\")\n",
        "\n",
        "        print(\"Merging tiles ...\")\n",
        "        filename = os.path.join(output_dir, map_name)\n",
        "        merge_tiles(os.path.join(temp_dir, f'*.{filetype}'), filename, bounding_box_original, crs, temp_dir, filetype)\n",
        "        print(\"... complete\")\n",
        "\n",
        "        # Move any downloaded files to the cache folder\n",
        "        if not os.path.exists(cache_dir):\n",
        "            os.makedirs(cache_dir)\n",
        "        for file in os.listdir(temp_dir):\n",
        "            if file.endswith(f'.{filetype}'):\n",
        "                shutil.move(os.path.join(temp_dir, file), os.path.join(cache_dir, file))\n",
        "\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "    return filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ERvzmSHEh_9e"
      },
      "outputs": [],
      "source": [
        "#@title Create maps from tiles\n",
        "\n",
        "#from map_from_tiles import create_map\n",
        "\n",
        "import geopandas as gpd\n",
        "import os\n",
        "\n",
        "include_DEM = True # @param {type:\"boolean\"}\n",
        "\n",
        "if include_DEM:\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    from osgeo import gdal\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Check if richdem is installed\n",
        "    try:\n",
        "        import richdem as rd\n",
        "    except ImportError:\n",
        "        !pip install richdem\n",
        "        import richdem as rd\n",
        "\n",
        "# Open the GeoPackage\n",
        "regions_gdf = gpd.read_file(geopackage_path, layer='regions')\n",
        "crs = regions_gdf.crs\n",
        "\n",
        "# Loop through each region in the GeoDataFrame\n",
        "for index, row in regions_gdf.iterrows():\n",
        "    # Extract the region name, URL, and other attributes\n",
        "    region_name = row['name']\n",
        "    geom = row['geometry']\n",
        "\n",
        "    # Get the extent (bounding box) of the geometry\n",
        "    extent = geom.bounds\n",
        "\n",
        "    map_filename = f\"{region_name}.jpg\"\n",
        "    map_path = os.path.join(map_directory, map_filename)\n",
        "    # Check if the map file already exists\n",
        "    if not os.path.exists(map_path):\n",
        "        # If it doesn't exist, create the map\n",
        "        map_path = create_map(basemap_url, map_directory, map_filename, extent, 17, crs, temp_directory, cache_directory)\n",
        "    else:\n",
        "        print(f\"The map for '{region_name}' already exists at '{map_directory}'. Skipping map creation.\")\n",
        "\n",
        "    map_one_inch_filename = f\"{region_name}.png\"\n",
        "    map_one_inch_path = os.path.join(map_one_inch_directory, map_one_inch_filename)\n",
        "    # Check if the map file already exists\n",
        "    if not os.path.exists(map_one_inch_path):\n",
        "        # If it doesn't exist, create the map\n",
        "        map_one_inch_path = create_map(basemap_url_one_inch, map_one_inch_directory, map_one_inch_filename, extent, 16, crs, temp_directory, cache_directory)\n",
        "    else:\n",
        "        print(f\"The one-inch map for '{region_name}' already exists at '{map_one_inch_directory}'. Skipping map creation.\")\n",
        "\n",
        "    if include_DEM:\n",
        "        filetype = dem_tilesource.split('.')[-1].split('?')[0]\n",
        "        if filetype == 'webp':\n",
        "            filetype = 'png'\n",
        "        map_elevation_filename = f\"{region_name}_elevation.{filetype}\"\n",
        "        map_elevation_path = os.path.join(map_elevation_directory, map_elevation_filename)\n",
        "        # Check if the map elevation file already exists\n",
        "        if not os.path.exists(map_elevation_path):\n",
        "            map_elevation_path = create_map(dem_tilesource, map_elevation_directory, map_elevation_filename, extent, dem_max_zoom, crs, temp_directory, cache_directory, filetype)\n",
        "        else:\n",
        "            print(f\"The elevation map for '{region_name}' already exists at '{map_elevation_directory}'. Skipping map elevation creation.\")\n",
        "\n",
        "        map_slope_filename = f\"{region_name}_slope.npy\"\n",
        "        map_slope_path = os.path.join(map_slope_directory, map_slope_filename)\n",
        "        # Check if the map slope file already exists\n",
        "        if not os.path.exists(map_slope_path):\n",
        "\n",
        "            # Rather convoluted due to difficulty in persuading richdem to load data any other way\n",
        "\n",
        "            # Open the original map image with GDAL\n",
        "            map_ds = gdal.Open(map_path)\n",
        "            map_width = map_ds.RasterXSize\n",
        "            map_height = map_ds.RasterYSize\n",
        "\n",
        "            # Open the elevation image with GDAL\n",
        "            elevation_ds = gdal.Open(map_elevation_path)\n",
        "\n",
        "            # Read the RGB values from the GDAL dataset\n",
        "            r_band = elevation_ds.GetRasterBand(1).ReadAsArray().astype(np.float32)\n",
        "            g_band = elevation_ds.GetRasterBand(2).ReadAsArray().astype(np.float32)\n",
        "            b_band = elevation_ds.GetRasterBand(3).ReadAsArray().astype(np.float32)\n",
        "\n",
        "            print(f\"Red: {np.min(r_band)} to {np.max(r_band)} ({np.mean(r_band)}); Green: {np.min(g_band)} to {np.max(g_band)} ({np.mean(g_band)}); Blue: {np.min(b_band)} to {np.max(b_band)} ({np.mean(b_band)}); \")\n",
        "\n",
        "            # Convert RGB values to elevation in npy array\n",
        "            elevation_map = np.array(-10000 + ((r_band * 256 * 256 + g_band * 256 + b_band) * .1)).astype(np.float32)\n",
        "            ## -10000 + ((red * 256 * 256 + green * 256 + blue) * 0.1);\n",
        "\n",
        "            # Get geotransform and projection from the JPG dataset\n",
        "            geotransform = list(elevation_ds.GetGeoTransform())\n",
        "            projection = elevation_ds.GetProjection()\n",
        "\n",
        "            # Create a GDAL in-memory dataset\n",
        "            in_mem_driver = gdal.GetDriverByName('MEM')\n",
        "            in_mem_ds = in_mem_driver.Create('', elevation_map.shape[1], elevation_map.shape[0], 1, gdal.GDT_Float32)\n",
        "\n",
        "            # Set geotransform and projection\n",
        "            in_mem_ds.SetGeoTransform(geotransform)\n",
        "            in_mem_ds.SetProjection(projection)\n",
        "\n",
        "            # Write data to the raster band\n",
        "            band = in_mem_ds.GetRasterBand(1)\n",
        "            band.WriteArray(elevation_map)\n",
        "\n",
        "            # Create a virtual file and write the in-memory dataset to it\n",
        "            virtual_file_path = '/vsimem/in_mem_dataset.tif'\n",
        "            gdal.GetDriverByName('GTiff').CreateCopy(virtual_file_path, in_mem_ds)\n",
        "\n",
        "            # Open the virtual file with richdem.LoadGDAL\n",
        "            rdarray = rd.LoadGDAL(virtual_file_path, no_data=-9999)\n",
        "\n",
        "            # Calculate slope array in degrees\n",
        "            slope_array = rd.TerrainAttribute(rdarray, attrib='slope_degrees')\n",
        "\n",
        "            # Resize slope array to map image dimensions using bicubic resampling\n",
        "            slope_image = Image.fromarray(slope_array)\n",
        "            resized_slope_image = slope_image.resize((map_width, map_height), Image.BICUBIC)\n",
        "            resized_slope_array = np.array(resized_slope_image)\n",
        "\n",
        "            # Clip slope array at 45 degrees and normalize to 0-255\n",
        "            slope_array_clipped = np.clip((resized_slope_array / 45) * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "            # Save to map_slope_path\n",
        "            np.save(map_slope_path, slope_array_clipped)\n",
        "\n",
        "            # Print summary of slope array\n",
        "            print(f\"Elevation for '{region_name}': {np.min(elevation_map)} to {np.max(elevation_map)}; Mean: {np.mean(elevation_map)}\")\n",
        "            print(f\"Slope for '{region_name}': {np.min(slope_array)} to {np.max(slope_array)}; Mean: {np.mean(slope_array)}\")\n",
        "\n",
        "            # Visualize Elevation Map\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(elevation_map, cmap='terrain', vmin=np.min(elevation_map), vmax=np.max(elevation_map))\n",
        "            plt.colorbar(label='Elevation (meters)')\n",
        "            plt.title(f'Elevation Map for {region_name}')\n",
        "            plt.show()\n",
        "\n",
        "            # Visualize Slope Array\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(slope_array_clipped, cmap='viridis', vmin=0, vmax=255)\n",
        "            plt.colorbar(label='Slope')\n",
        "            plt.title(f'Slope Map for {region_name}')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"The slope map for '{region_name}' already exists at '{map_slope_directory}'. Skipping map slope creation.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HxKEKBtFNqBR"
      },
      "outputs": [],
      "source": [
        "#@title Fetch OSM modern features\n",
        "\n",
        "# Check if OverPy is installed\n",
        "try:\n",
        "    import overpy\n",
        "except ImportError:\n",
        "    !pip install overpy\n",
        "    import overpy\n",
        "\n",
        "# Check if Rasterio is installed\n",
        "try:\n",
        "    import rasterio\n",
        "except ImportError:\n",
        "    !pip install rasterio\n",
        "    import rasterio\n",
        "\n",
        "from rasterio.transform import from_origin\n",
        "from rasterio.features import geometry_mask, rasterize\n",
        "import rasterio.transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.ops import cascaded_union\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, LineString, Polygon, mapping, box, shape\n",
        "import os\n",
        "import pyproj\n",
        "\n",
        "def convert_coords_to_4326(min_x, min_y, max_x, max_y, crs):\n",
        "    # Create a Pyproj transformer\n",
        "    transformer = pyproj.Transformer.from_crs(crs, 'EPSG:4326', always_xy=True)\n",
        "\n",
        "    # Transform the corners of the bounding box\n",
        "    min_lon, min_lat = transformer.transform(min_x, min_y)\n",
        "    max_lon, max_lat = transformer.transform(max_x, max_y)\n",
        "\n",
        "    return min_lat, min_lon, max_lat, max_lon\n",
        "\n",
        "def fetch_osm_gdf(min_lat, min_lon, max_lat, max_lon):\n",
        "    api = overpy.Overpass()\n",
        "\n",
        "    query = f\"\"\"\n",
        "    (\n",
        "      way[\"highway\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
        "      way[\"railway\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
        "      (\n",
        "        way[\"waterway\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
        "        way[\"natural\"=\"water\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
        "        way[\"landuse\"~\"^(reservoir|basin|pond|lake)$\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
        "      );\n",
        "    );\n",
        "    (._;>;);\n",
        "    out geom;\n",
        "    \"\"\"\n",
        "\n",
        "    result = api.query(query)\n",
        "\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        {\n",
        "            \"geometry\": [\n",
        "                Polygon([[float(node.lon), float(node.lat)] for node in way.nodes])\n",
        "                if way.nodes[0] == way.nodes[-1]\n",
        "                else LineString([[float(node.lon), float(node.lat)] for node in way.nodes])\n",
        "                for way in result.ways\n",
        "            ],\n",
        "            \"class\": [\n",
        "                \"motorway\" if \"highway\" in way.tags and \"motorway\" in way.tags[\"highway\"]\n",
        "                else \"rail\" if \"highway\" in way.tags and \"rail\" in way.tags[\"highway\"]\n",
        "                else \"road\" if \"highway\" in way.tags\n",
        "                else \"rail\" if \"railway\" in way.tags\n",
        "                else \"water\" if \"waterway\" in way.tags or (\n",
        "                    \"natural\" in way.tags and way.tags[\"natural\"] == \"water\"\n",
        "                ) or (\n",
        "                    \"landuse\" in way.tags\n",
        "                    and way.tags[\"landuse\"] in [\"reservoir\", \"basin\", \"pond\", \"lake\"]\n",
        "                )\n",
        "                else None\n",
        "                for way in result.ways\n",
        "            ],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Filter out rows with class as None\n",
        "    gdf = gdf.dropna(subset=[\"class\"])\n",
        "\n",
        "    return gdf\n",
        "\n",
        "# Open the GeoPackage\n",
        "regions_gdf = gpd.read_file(geopackage_path, layer='regions')\n",
        "labels_gdf = gpd.read_file(geopackage_path, layer='labels')\n",
        "crs = regions_gdf.crs\n",
        "\n",
        "# Loop through each region in the GeoDataFrame\n",
        "for index, row in regions_gdf.iterrows():\n",
        "    # Extract the region name, URL, and other attributes\n",
        "    region_name = row['name']\n",
        "    geom = row['geometry']\n",
        "\n",
        "    osm_output_path = f\"{map_osm_directory}{region_name}_osm.npy\"\n",
        "    if os.path.exists(osm_output_path):\n",
        "        print(f\"The OSM data for '{region_name}' already exist at '{osm_output_path}'. Skipping creation.\")\n",
        "    else:\n",
        "\n",
        "        # Get the extent (bounding box) of the geometry\n",
        "        extent = geom.bounds\n",
        "        min_lat, min_lon, max_lat, max_lon = convert_coords_to_4326(*extent, crs)\n",
        "\n",
        "        osm_gdf = fetch_osm_gdf(min_lat, min_lon, max_lat, max_lon)\n",
        "        # Crop to extent\n",
        "        osm_gdf = gpd.clip(osm_gdf, box(min_lon, min_lat, max_lon, max_lat)) # Swap lat<->lon\n",
        "        osm_gdf = osm_gdf[~osm_gdf.is_empty]\n",
        "        osm_gdf = osm_gdf.explode(index_parts=True)  # Cropping can cause creation of multiparts\n",
        "\n",
        "        map_path = os.path.join(map_directory, f\"{region_name}.jpg\")\n",
        "        with rasterio.open(map_path) as map_ds:\n",
        "            transform_3857 = map_ds.transform\n",
        "            array_shape = map_ds.shape\n",
        "\n",
        "        # Function to convert coordinates to pixel values\n",
        "        def coords_to_pixel(transform_3857, lon, lat):\n",
        "            lon_3857, lat_3857 = pyproj.Transformer.from_crs('EPSG:4326', 'EPSG:3857').transform(lat, lon) # Swap lat<->lon\n",
        "            y, x = rasterio.transform.rowcol(transform_3857, lon_3857, lat_3857)\n",
        "            return int(x), int(y)\n",
        "\n",
        "        def transform_coords(geom, transform_3857):\n",
        "            if geom.geom_type == 'Polygon':\n",
        "                # Apply the transformation to the exterior ring\n",
        "                exterior_coords = [coords_to_pixel(transform_3857, *coord) for coord in geom.exterior.coords]\n",
        "\n",
        "                # Apply the transformation to each interior ring (hole)\n",
        "                interior_coords = [\n",
        "                    [coords_to_pixel(transform_3857, *coord) for coord in interior.coords]\n",
        "                    for interior in geom.interiors\n",
        "                ]\n",
        "\n",
        "                return Polygon(exterior_coords, interior_coords)\n",
        "            elif geom.geom_type == 'LineString':\n",
        "                # Apply the transformation to the LineString\n",
        "                return LineString([coords_to_pixel(transform_3857, *coord) for coord in geom.coords])\n",
        "            else:\n",
        "                # Handle other geometry types as needed\n",
        "                return None\n",
        "\n",
        "        osm_gdf['geometry'] = osm_gdf['geometry'].apply(lambda geom: transform_coords(geom, transform_3857))\n",
        "\n",
        "        # Function to buffer LineStrings and collect all geometries\n",
        "        def buffer_and_collect(row, buffer_width):\n",
        "            geometry = row['geometry']\n",
        "\n",
        "            if isinstance(geometry, LineString):\n",
        "                buffer_multiplier = 2 if row['class'] in ['motorway', 'rail'] else 1\n",
        "                geometry = geometry.buffer(buffer_width * buffer_multiplier, cap_style=2)\n",
        "\n",
        "            return geometry\n",
        "\n",
        "        # Create a dictionary to store arrays for each class\n",
        "        class_arrays = {'motorway': None, 'road': None, 'rail': None, 'water': None}\n",
        "\n",
        "        # Iterate over unique classes\n",
        "        for class_value in class_arrays.keys():\n",
        "            # Filter rows for the current class\n",
        "            class_rows = osm_gdf[osm_gdf['class'] == class_value]\n",
        "\n",
        "            # Buffer LineStrings and collect all geometries into a list\n",
        "            geometries = class_rows.apply(lambda row: buffer_and_collect(row, linestring_buffer), axis=1).tolist()\n",
        "\n",
        "            if len(geometries) > 0:\n",
        "\n",
        "                # Create an array for the current class\n",
        "                class_array = rasterio.features.rasterize(\n",
        "                    shapes=[(geom, 255) for geom in geometries],\n",
        "                    out_shape=array_shape,\n",
        "                    fill=0,\n",
        "                    all_touched=True,\n",
        "                    merge_alg=rasterio.enums.MergeAlg.replace,\n",
        "                    dtype=np.uint8\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                class_array = np.zeros(array_shape, dtype=np.uint8)\n",
        "\n",
        "            # Store the array in the dictionary with the class as the key\n",
        "            class_arrays[class_value] = class_array\n",
        "\n",
        "        stacked_array = np.stack(list(class_arrays.values()), axis=0)\n",
        "        np.save(f\"{map_osm_directory}{region_name}_osm.npy\", stacked_array)\n",
        "\n",
        "        # Iterate over each class and its corresponding array\n",
        "        for i, class_value in enumerate(class_arrays.keys()):\n",
        "            class_array = stacked_array[i, :, :]\n",
        "\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(class_array, cmap='gray', interpolation='none')\n",
        "            plt.title(f'{region_name} {class_value}')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cwKLxX6yI1Rw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cj2Adb1FYVQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8b9422-c56b-440c-f3fb-1f4ad597a8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 labels found: ['Blue', 'Red', 'Shading', 'Road Spot', 'Contour Spot', 'Road', 'Boundary Spot', 'Tree Broadleaf', 'Tree Conifer', 'Hatch', 'Line 5px', 'Line 3px', 'Dash 3px', 'Black Solid', 'Road Name', 'Background', 'Embankment', 'Marsh', 'Water', 'Railway'].\n",
            "Ilastik is not installed. Installing...\n",
            "... done.\n",
            "Classifying Shardlow.jpg...\n",
            "Classifying Tormarton.jpg...\n",
            "Classifying Lothersdale.jpg...\n",
            "Classifying Tolleshunt_Major.jpg...\n",
            "Classifying Snowdon.jpg...\n",
            "Classifying Walsall.jpg...\n",
            "Classifying Bristol.jpg...\n",
            "Classifying Llanfynydd.jpg...\n",
            "Classifying Bolam.jpg...\n",
            "Classifying Norton.jpg...\n",
            "Classifying Oundle.jpg...\n",
            "Classifying Stilton.jpg...\n",
            "Classifying Water_Orton.jpg...\n",
            "Classifying Bath.jpg...\n",
            "Classifying Swindon.jpg...\n",
            "Classifying Northampton.jpg...\n",
            "Classifying Papworth.jpg...\n",
            "Classifying Nottingham.jpg...\n",
            "Classifying Winchester.jpg...\n",
            "Classifying Salisbury.jpg...\n",
            "Classifying York.jpg...\n",
            "Classifying Belsay.jpg...\n",
            "Classifying Ripon.jpg...\n"
          ]
        }
      ],
      "source": [
        "#@title Ilastik: classify pixels (Stage 1)\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "# import numpy as np\n",
        "# import shutil\n",
        "# import importlib\n",
        "\n",
        "reprocess_maps = True # @param {type:\"boolean\"}\n",
        "dataset_name = \"probabilities\" # User's choice\n",
        "export_source = \"Probabilities\" # Could alternatively be \"Simple Segmentation\"\n",
        "export_dtype = \"float32\" # Should be \"float32\" for probablilities (\"uint8\" does not cause normalisation to 0-255)\n",
        "output_format = \"numpy\" # Could be \"hdf5\"\n",
        "\n",
        "with h5py.File(ilastik_project_file, 'r') as f:\n",
        "    labels = f['PixelClassification']['LabelNames'][:]\n",
        "    label_strings = [label.decode('utf-8') for label in labels]\n",
        "\n",
        "    print(f\"{len(label_strings)} labels found: {label_strings}.\")\n",
        "\n",
        "    # Save label_strings to a file in the model_directory\n",
        "    with open(label_strings_file, 'w') as file:\n",
        "        for label in label_strings:\n",
        "            file.write(label + '\\n')\n",
        "\n",
        "def preprocess_map(jpg_filename):\n",
        "    jpg_path = os.path.join(map_directory, jpg_filename)\n",
        "    jpg_georeference = os.path.join(map_directory, jpg_filename.replace('.jpg', '.jpg.aux.xml'))\n",
        "    classified_s1_filename = jpg_filename.replace('.jpg','.classified_s1.npy')\n",
        "    classified_s1_path = os.path.join(map_classified_s1_directory, classified_s1_filename)\n",
        "\n",
        "    # Check if the corresponding .npy file exists\n",
        "    if reprocess_maps or not os.path.exists(classified_s1_path):\n",
        "\n",
        "        # Check if Ilastik is already installed\n",
        "        if not os.path.exists(ilastik_executable):\n",
        "            print(\"Ilastik is not installed. Installing...\")\n",
        "            install_ilastik()\n",
        "            print(\"... done.\")\n",
        "\n",
        "        # Run ilastik for the current .jpg file\n",
        "        print(f\"Classifying {jpg_filename}...\")\n",
        "        command = f\"{ilastik_executable} --headless \" \\\n",
        "                  f\"--project='{ilastik_project_file}' \" \\\n",
        "                  f\"--output_format='{output_format}' \" \\\n",
        "                  f\"--output_filename_format='{classified_s1_path}' \" \\\n",
        "                  f\"--output_internal_path='/{dataset_name}' \" \\\n",
        "                  f\"--export_source='{export_source}' \" \\\n",
        "                  f\"--export_dtype='{export_dtype}' \" \\\n",
        "                  f\"'{jpg_path}'\"\n",
        "        os.system(command)\n",
        "\n",
        "# Iterate through the .jpg files in the directory and preprocess each map\n",
        "jpg_filenames = [preprocess_map(jpg_filename) for jpg_filename in os.listdir(map_directory) if jpg_filename.endswith('.jpg')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T3ZcsAKuI6hK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NwGo3YE_UjV",
        "outputId": "6b938672-67ad-49e6-d1ef-48dcfd9e5a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 labels found: ['Blue', 'Red', 'Shading', 'Road Spot', 'Contour Spot', 'Road', 'Boundary Spot', 'Tree Broadleaf', 'Tree Conifer', 'Hatch', 'Line 5px', 'Line 3px', 'Dash 3px', 'Black Solid', 'Road Name', 'Background', 'Embankment', 'Marsh', 'Water', 'Railway'].\n",
            "Generating Shardlow.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Shardlow.augmented_s1.npy: (2755, 4592, 28)\n",
            "Generating Tormarton.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Tormarton.augmented_s1.npy: (1766, 2782, 28)\n",
            "Generating Lothersdale.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Lothersdale.augmented_s1.npy: (1676, 2423, 28)\n",
            "Generating Tolleshunt_Major.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Tolleshunt_Major.augmented_s1.npy: (2286, 2845, 28)\n",
            "Generating Snowdon.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Snowdon.augmented_s1.npy: (2350, 2984, 28)\n",
            "Generating Walsall.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Walsall.augmented_s1.npy: (1341, 1453, 28)\n",
            "Generating Bristol.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Bristol.augmented_s1.npy: (1787, 2061, 28)\n",
            "Generating Llanfynydd.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Llanfynydd.augmented_s1.npy: (1722, 2618, 28)\n",
            "Generating Bolam.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Bolam.augmented_s1.npy: (1113, 1987, 28)\n",
            "Generating Norton.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Norton.augmented_s1.npy: (1447, 2805, 28)\n",
            "Generating Oundle.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Oundle.augmented_s1.npy: (1430, 2333, 28)\n",
            "Generating Stilton.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Stilton.augmented_s1.npy: (1426, 1842, 28)\n",
            "Generating Water_Orton.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Water_Orton.augmented_s1.npy: (2838, 5539, 28)\n",
            "Generating Bath.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Bath.augmented_s1.npy: (626, 782, 28)\n",
            "Generating Swindon.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Swindon.augmented_s1.npy: (1200, 1661, 28)\n",
            "Generating Northampton.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Northampton.augmented_s1.npy: (1109, 1736, 28)\n",
            "Generating Papworth.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Papworth.augmented_s1.npy: (2461, 4718, 28)\n",
            "Generating Nottingham.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Nottingham.augmented_s1.npy: (1238, 1116, 28)\n",
            "Generating Winchester.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Winchester.augmented_s1.npy: (1398, 1572, 28)\n",
            "Generating Salisbury.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Salisbury.augmented_s1.npy: (832, 1131, 28)\n",
            "Generating York.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/York.augmented_s1.npy: (680, 1160, 28)\n",
            "Generating Belsay.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Belsay.augmented_s1.npy: (2025, 4168, 28)\n",
            "Generating Ripon.augmented_s1.npy...\n",
            "Shape of data in /content/drive/MyDrive/desCartes/training_data/maps/augmented_s1/Ripon.augmented_s1.npy: (914, 937, 28)\n"
          ]
        }
      ],
      "source": [
        "#@title Generate Inputs from Maps & Distance Tables\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import importlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "foreground_labels = ['Blue', 'Hatch', 'Line 5px', 'Line 3px', 'Black Solid']\n",
        "foreground_indices = []\n",
        "\n",
        "with h5py.File(ilastik_project_file, 'r') as f:\n",
        "    labels = f['PixelClassification']['LabelNames'][:]\n",
        "    label_strings = [label.decode('utf-8') for label in labels]\n",
        "    for foreground_label in foreground_labels:\n",
        "        # Get the index for the current target label\n",
        "        label_index = label_strings.index(foreground_label)\n",
        "        foreground_indices.append(label_index)\n",
        "\n",
        "    print(f\"{len(label_strings)} labels found: {label_strings}.\")\n",
        "\n",
        "    # Save label_strings to a file in the model_directory\n",
        "    with open(label_strings_file, 'w') as file:\n",
        "        for label in label_strings:\n",
        "            file.write(label + '\\n')\n",
        "\n",
        "def preprocess_map(jpg_filename):\n",
        "    jpg_path = os.path.join(map_directory, jpg_filename)\n",
        "    jpg_georeference = os.path.join(map_directory, jpg_filename.replace('.jpg', '.jpg.aux.xml'))\n",
        "    classified_s1_filename = jpg_filename.replace('.jpg','.classified_s1.npy')\n",
        "    classified_s1_path = os.path.join(map_classified_s1_directory, classified_s1_filename)\n",
        "    augmented_s1_filename = jpg_filename.replace('.jpg','.augmented_s1.npy')\n",
        "    augmented_s1_path = os.path.join(map_augmented_s1_directory, augmented_s1_filename)\n",
        "    slope_filename = jpg_filename.replace('.jpg','_slope.npy')\n",
        "    slope_path = os.path.join(map_slope_directory, slope_filename)\n",
        "    one_inch_filename = jpg_filename.replace('.jpg','.png')\n",
        "    one_inch_path = os.path.join(map_one_inch_directory, one_inch_filename)\n",
        "    osm_filename = jpg_filename.replace('.jpg','_osm.npy')\n",
        "    osm_path = os.path.join(map_osm_directory, osm_filename)\n",
        "    binary_filename = jpg_filename.replace('.jpg','.binary.png')\n",
        "    binary_path = os.path.join(map_binary_directory, binary_filename)\n",
        "    skeleton_filename = jpg_filename.replace('.jpg', '.skeleton.png')\n",
        "    skeleton_path = os.path.join(map_skeleton_directory, skeleton_filename)\n",
        "\n",
        "    print(f\"Generating {augmented_s1_filename}...\")\n",
        "\n",
        "    if os.path.exists(classified_s1_path):\n",
        "        # Load the data from the .classified_s1.npy file\n",
        "        data = np.load(classified_s1_path)\n",
        "\n",
        "        if data.shape[-1] == len(label_strings):\n",
        "\n",
        "            # Extract Ilastik probabilities for all classes and normalize to uint8\n",
        "            ilastik_probabilities = (data * 255).astype(np.uint8)\n",
        "\n",
        "            # Prepend the RGB channels to the 'ilastik_probabilities' array\n",
        "            rgb_image = np.array(Image.open(jpg_path))\n",
        "            augmented_data = np.concatenate((rgb_image[..., :3], ilastik_probabilities), axis=-1)\n",
        "\n",
        "            # Create a 'foreground' probability map and add as channel to augmented_data\n",
        "            foreground_prob_map = np.sum(ilastik_probabilities[..., foreground_indices], axis=-1)\n",
        "            augmented_data = np.concatenate((augmented_data, foreground_prob_map[..., np.newaxis]), axis=-1)\n",
        "\n",
        "            # Resize and append the One-Inch .png map as normalised grayscale to the 'data' array\n",
        "            one_inch_image = Image.open(one_inch_path).convert('L')\n",
        "            one_inch_image_resized = one_inch_image.resize((rgb_image.shape[1], rgb_image.shape[0]), Image.BICUBIC)  # Resize to match rgb_image\n",
        "            normalized_one_inch_array = np.array(one_inch_image_resized, dtype=np.float32)\n",
        "            min_value = np.min(normalized_one_inch_array)\n",
        "            max_value = np.max(normalized_one_inch_array)\n",
        "            normalized_one_inch_array = 255 * (normalized_one_inch_array - min_value) / (max_value - min_value)\n",
        "            one_inch_array = np.array(normalized_one_inch_array, dtype=np.uint8)\n",
        "            augmented_data = np.concatenate((augmented_data, one_inch_array[..., np.newaxis]), axis=-1)\n",
        "\n",
        "            # Append the DEM slope\n",
        "            slope_array = np.load(slope_path)\n",
        "            slope_array = slope_array[:, :, np.newaxis]\n",
        "            augmented_data = np.concatenate((augmented_data, slope_array), axis=2)\n",
        "\n",
        "            # Convert OSM modern network data arrays (4 classes) to distance maps clipped to 255, and concatenate to augmented_data\n",
        "            osm_array = np.moveaxis(np.load(osm_path), 0, -1) # 'motorway, 'road', 'rail', 'water'\n",
        "            osm_distance_maps = [np.clip(distance_transform_edt(255 - osm_array[..., i]), 0, 255) for i in [2, 3]] # Use only the rail and water classes\n",
        "            osm_distance_maps_stacked = np.stack(osm_distance_maps, axis=-1)\n",
        "            augmented_data = np.concatenate((augmented_data, osm_distance_maps_stacked), axis=-1)\n",
        "\n",
        "            np.save(augmented_s1_path, augmented_data)\n",
        "            print(f\"Shape of data in {augmented_s1_path}: {augmented_data.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"{classified_s1_path} does not exist.\")\n",
        "    return jpg_filename\n",
        "\n",
        "# Iterate through the .jpg files in the directory and preprocess each map\n",
        "jpg_filenames = [preprocess_map(jpg_filename) for jpg_filename in os.listdir(map_directory) if jpg_filename.endswith('.jpg')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UoxjoZ2_I-Jq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ee40ec0b256d417781b053894ca6c33f",
            "d7ff9120eda646b39b8bb83eea053264",
            "3c49848d21d746a7a8507dc9ac838068",
            "52368bbbb8cd43c2971f2f18596216eb",
            "5d48ddc3e6aa4091bead67c0e3d8731b",
            "203340faa96448f58fdbd94b4d963e18",
            "15553eccaee44a0bb5ae6a2aad9e4c63",
            "d29cca9a940d48a583cc1920be5ae714",
            "3333dde2908945b093f532c5e6d229cd",
            "7f38b0cb7aa043feb5c7af5803353b70",
            "8caf2d7ac56643e68f2e3eca7c5e76e2",
            "af400b60bc684be5a6f21138c2beb293",
            "d0942fb144554e4190e911d9260fe906",
            "dd652471e43346738da073d05c6e4f91",
            "b29798dc20b74da5953b40ec0805c3e1",
            "8410059792e1414785beb8c6873e8d70",
            "e9cde39f3cfc4333acb17c0bfd514f01",
            "1977491a3edc48a18a307e2e48ce396f",
            "6a1d2063f26843828cdc4f57c468c404",
            "bddffe4b61874474aa6afe74e3d65d51",
            "cf50a48588a647a0a70e4348077b11cb",
            "5cc0d5e2f2b24ffd8beb69e0e7b623e2",
            "8d1db6b7e9914a55be4d48d023b26196",
            "44cee4e817f34e7188343393775d101c",
            "4c9d1fe8027f4c0fbfb03a25c0daad75",
            "a140dd43df8546da94618fd2e96c93a8",
            "ffc772ea36964b1abb3ca79439954f42",
            "3d305502b791495ea92d067ae0e3efa4",
            "114138b3da464524b1299abcdebec87c",
            "25754daf42284988b1a1084165cd82fa",
            "8fdc0c7db3114e2680ac8f0df582ac1c",
            "9cb80b62256d4c7b825ad67ec448c9b1",
            "4029c6ee6da140db958a1291c4cb48f3",
            "37ef5c28ad9d4058ba40ee27a1586164",
            "ce5280ce9307449989dec74f8f566b35",
            "871293440d9446e4b0203d0a00099d64",
            "cd955f717ee047c09bd2608d9a2b6945",
            "c973ed62acd1477fa9e135e3f47a7461",
            "9624c00d4a7245259d3acd7af1c80e9e",
            "398a605cbf4d432681e002876d335b72",
            "cf80127aa6f14156bac54ea259a4bf44",
            "5b651e3e55714c2b92f13db75f21f622",
            "588b3f7220cd4ae8bc5336787f1a9496",
            "adb41f13cd934d46a869537cf78c43e6",
            "1275de2ceac44beab0c2ec51754a8c61",
            "fd52f44836184210bc3649ddc2ddc040",
            "abfa6de0835142baa290ff70a650f2ed",
            "537945cdec9c4d0a8beafd27097e3af5",
            "65d17ac8bcb6473e9f5b482db0797a03",
            "53964d4cf4824c8b8aeebf07b114bcb9",
            "456cdc312b334191b8153bcde9346bef",
            "075dcd0cd3004618b2c6a2fe9f3bcc3a",
            "885ccadd79b44f8ab2674ac40b7f332c",
            "9f97b49c3d9045d88e377600c9455f39",
            "e2af77e13bbf4bcb90b50957530d7b9e",
            "251dbe00bb2447efbfb1c041bdbed84d",
            "def757a289c04c7f8ca4f953fce50712",
            "3549017379eb4ca1a1617cfd13139153",
            "f53035d901cc42aabb57d993f971f97a",
            "21c02e8aa1ef4648b8813cc2ba8c0d57",
            "e76b790746e14264abf83515ab190a90",
            "b4203b2f732b441e8847271f68a7f871",
            "3055032c41634c04a687f5de5306c003",
            "80fd6ea3045d443dad0810235a031767",
            "31d50c1d0104472d99bc570eb97c636d",
            "f5a7499ba7a747859d77d28a706899ca",
            "d744654ee85748f2acda2274ad858d6f",
            "775b3cabd7c1490ca49891df30e1db9d",
            "e44e85b1c40e44ea9bce9eff9cda54d2",
            "b00a2488a95a45b79bc50a8e05971662",
            "55b267555dc6464f8bcd7840ba47e574",
            "2c8988b05efb4d4a9aca5550b62a22ee",
            "b8f4b94b0a404ef4a724b2c0fadedd4f",
            "aaa2508a911a423096a6c3e05d3b1e7d",
            "4f72e3a220c1450a8b0f2b89fa21241e",
            "a6f154ee8ae9484e85f3cd4a2a002d0a",
            "368ef66de14149ceb214ec7ae7f2c8c4",
            "3ab9ccde429d4d4e8c09a9ba7f879746",
            "3b8383272cc4411baddd88176a2ebfb5",
            "2c6f55f6cd14439d97e8be73bbcb3705",
            "cb1f388c61e84871a010fca376d9311a",
            "6f158f138a2a44dc9eaf24b8554296ce",
            "87d89c9dab1643e883ef8fad83571990",
            "75ebd104056a40e6a44cd40e20fcae39",
            "0b57e0fa60d44fa3954e276af20db20a",
            "54279b878df64db6a341044796bbf620",
            "fe765ae1b3e54ba9a2066678369ca626",
            "67b10190818b49a8adf551cc773e80e7",
            "5234ead62b1a4ea49293547df1c56b96",
            "d4ec45aed73b4a5694ac803c29995a46",
            "3872b555773c4ceface1974beba97591",
            "3c9a896b03ab49e09fc01aabacc2c7a7",
            "32529ad576ae4f76ad690d21646dbe92",
            "decbe7354f624c8a920e98e30b31ae26",
            "fc2a6b39780a4294b9fbf246d929b34d",
            "633999699c6a4d58a8d94ffaec8205b9",
            "2f795316accf4e7e904377977ff877e4",
            "38e1d8931e0d447a987f0e3b71693bcd",
            "2da803eaa053453e9f05c22a18b88b86",
            "accc617c4b81439f971d1b2bfd3ff8f3",
            "4fba00a3f169457f931c2f01faefdc13",
            "4a05e365d27b40db8f6e2c6182c5ccfb",
            "8dab659aa84c4d0383404020dfb16534",
            "402a61e2827647a7967ced62d3c17a68",
            "ea61ba2acc8445febec95103feb6c231",
            "2b5656b04e2d4b44bc8883a1dbdb17be",
            "50621f006672427b981af22b30957bb5",
            "ffe17c256dd54e1382804b9da849959d",
            "da564a4e4adb4f82a72a5289c0bd0182",
            "53fa424b1e1444919eb1b3de4bded0e2",
            "f3c18ed1ca8a499c842954cd3b34e3a9",
            "4694805dd1714f89aaf027446d1290b8",
            "ea43416f7e4a4badaf5da1858cea59c8",
            "56ea9dac85ea423c8e050ab9d5af73c1",
            "708eb13974344f61b3265d840b07fe69",
            "718b1b0f40014b72b5eb168180c7b6fc",
            "1f0e87191a70467f85821f6321a48070",
            "eccd3d24e02b4daf9e0a01005b98e251",
            "3c5489f1c21a4d2bbee6252b7077e422",
            "26c10ee8f33b42dfad68c1f4c9683de7",
            "bdedbbe38f1540d1b19aee4599810518",
            "e7a1e7c584df4c26bc9815e1e3deb4fc",
            "4b6da87baf7c444bac662d6dfe4166a1",
            "f1f4be0524c04eec94a44bc1cc1374b9",
            "2c8a0bd6c8204c3fba0f8680ed5fec4b",
            "7350d18b73264147ad86c672fe8dc7fb",
            "9bb96a0952a54c2a9f1ae43138191431",
            "b7c263fce61743d5bf093eae2edee03d",
            "361584b51bdc4ceea8eadc7ba80e35ff",
            "a4c4c1fc4481444f9ace8d79c195ef42",
            "f22fcc5cafcb489ea7b29154c924eefb",
            "af5553b342bf49d59c3783893a5c2ad9",
            "cfb92f8574c64049b703cdf56399ebb7",
            "c42bb81736224a2bb232bfdb39ff16f2",
            "770c55e9265247b98336e9fb9b656e0d",
            "8905bf66e9794e078dc02dced0575b6f",
            "c7c0599809a446dc8b2821c670db2f23",
            "a43ad0f07a0a48d1bafef37839665f48",
            "285608e93b7a45bba91ca5f88d51e0b2",
            "bfb83fc3728b40049d4131117785f357",
            "081554494ed842f89db927b3982d4cfc",
            "adcbc6fa297246068c8f218aacecc334",
            "d2b6b10bdf0e47e291f3eb6a114b58a5",
            "c8917c7b178e4429b431b9f17fb50ac1",
            "ccda98f4b9ca4947ab01f784db8f9397",
            "6a58d23376e6457ba99d5b09a93f22bc",
            "7af0906c20bc4535a7fdecfa900a1262",
            "c846b95eb62f4f02b8bbf194a070c7bb",
            "60a3aaf7a35643d59b92ae41a5ddb63f",
            "54c7827b753d48f9bc89749a4cd3b273",
            "a36b548fbeaa42ceb3b218e3520d2f79",
            "0257af240fcf469a98afbaa51c9c854c",
            "ce6f5a77de4646bcb08481795d5fb39b",
            "d1a152ea7d57440f9498d166b5592699",
            "9f57801fc3114a6db9f071d166bc8774",
            "7b1c005e0d394a11b7ddd7ec9bd40ecf",
            "71b38f00945a4a07b9c43ce2542df612",
            "cfaa1c5e1e1b4e7b8ef6caf175cfca3f",
            "9ba3e6c706bc4908be36bdbe7bd89d50",
            "31d667b927f4475d85cf515196d3f4b2",
            "95ab54a313544841af5f9524b36fd5b7",
            "2b4b5fcbecc245a794dbb15b702a3956",
            "263a2fcc96fb4c4385c36698e96718df",
            "d4f6b9f267284524a1a1d27fe4255b9b",
            "75b008ca126c4a508ea57557a84d8ae8",
            "4152f846ecc8497fa4342a523a70cb26",
            "dc28d8fb1ec6454dbd224c754f75538f",
            "304e9d5f68034b92987363ce9011f306",
            "021b169b907248e1b43dab77810f5930",
            "cb511b7140774eca943c0b00a3521d37",
            "db455292ca3c4ac58aa371cd90a62976",
            "8ff164cbf5834bf7b6c7e1ae58405541",
            "f47dfc16ebe54a3ba99d7807547f83d6",
            "25adf040df8646f4b9c08ba963747a4d",
            "91f30d54baf845d9aee442ed55f79766",
            "4b85e5c10f48476b9a4e7d9fa18b5070",
            "4cf70eb555eb47afba98f7363192d911",
            "895a40f0e59a4b62ac016525386a0ad9",
            "9ac40a60606d412ca61c4d8413d44ef2",
            "536cc28fb59a429385d1aefd66a54cc7",
            "5caa8950a5614317a5c97906d5611fb6",
            "b7463d16f29b4dac8c03503b4c01f8f2",
            "8550449b12ec4138a04bc0bc3d260700",
            "d2513472e73346b69bea63da0c642c21",
            "e271004317a94e2db14adcb55c76b38c",
            "87d4b4602e8e47ebbd0094912aca2807",
            "d8470c63f990497f98630b3daa85b04c"
          ]
        },
        "id": "SvbuJ438NKZ3",
        "outputId": "16044acb-9fad-4a5e-e12c-c94477626951",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.9-cp310-cp310-manylinux2014_x86_64.whl (20.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2023.11.17)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.23.5)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.9 snuggs-1.4.7\n",
            "map_width: 4592, map_height: 2755, horizontal_overlap: 27.789473684210527, vertical_overlap: 28.818181818181817\n",
            "Labelling Shardlow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 9 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 10 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 11 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 31 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 32 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 33 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 50 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 106 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Shardlow:   0%|          | 0/240 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee40ec0b256d417781b053894ca6c33f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20427640871022645, 1: 19.814236493849243, 2: 34.21426551521611, 3: 101.25951200669542, 4: 66.22445843245406}\n",
            "map_width: 2782, map_height: 1766, horizontal_overlap: 26.363636363636363, vertical_overlap: 40.285714285714285\n",
            "Labelling Tormarton.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Tormarton:   0%|          | 0/96 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af400b60bc684be5a6f21138c2beb293"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20527110022965653, 1: 14.689269275412592, 2: 38.474810640807235, 3: 85.45354211537342, 4: 44.2019712146456}\n",
            "map_width: 2423, map_height: 1676, horizontal_overlap: 39.3, vertical_overlap: 19.333333333333332\n",
            "Labelling Lothersdale.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Lothersdale:   0%|          | 0/77 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d1db6b7e9914a55be4d48d023b26196"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20545009190893568, 1: 15.64787725145472, 2: 31.318840110157137, 3: 73.45111331225661, 4: 43.1271249770951}\n",
            "map_width: 2845, map_height: 2286, horizontal_overlap: 20.636363636363637, vertical_overlap: 30.444444444444443\n",
            "Labelling Tolleshunt_Major.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Tolleshunt_Major:   0%|          | 0/120 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37ef5c28ad9d4058ba40ee27a1586164"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.2050956366542544, 1: 15.777149554766137, 2: 38.28314281643523, 3: 68.33210352315186, 4: 49.78221671155956}\n",
            "map_width: 2984, map_height: 2350, horizontal_overlap: 28.666666666666668, vertical_overlap: 23.333333333333332\n",
            "Labelling Snowdon.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Snowdon:   0%|          | 0/130 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1275de2ceac44beab0c2ec51754a8c61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20445254823592354, 1: 19.62523481202617, 2: 47.62049472288285, 3: 80.59348951087864, 4: 40.7707128949776}\n",
            "map_width: 1453, map_height: 1341, horizontal_overlap: 16.6, vertical_overlap: 39.0\n",
            "Labelling Walsall.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Walsall:   0%|          | 0/36 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "251dbe00bb2447efbfb1c041bdbed84d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20538581050269408, 1: 20.690858421728947, 2: 22.93602499399183, 3: 73.82999153874049, 4: 39.00188497722532}\n",
            "map_width: 2061, map_height: 1787, horizontal_overlap: 30.375, vertical_overlap: 37.285714285714285\n",
            "map_width: 2618, map_height: 1722, horizontal_overlap: 19.8, vertical_overlap: 46.57142857142857\n",
            "map_width: 1987, map_height: 1113, horizontal_overlap: 39.625, vertical_overlap: 41.75\n",
            "map_width: 2805, map_height: 1447, horizontal_overlap: 24.272727272727273, vertical_overlap: 17.8\n",
            "map_width: 2333, map_height: 1430, horizontal_overlap: 25.22222222222222, vertical_overlap: 21.2\n",
            "map_width: 1842, map_height: 1426, horizontal_overlap: 29.428571428571427, vertical_overlap: 22.0\n",
            "Labelling Stilton.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Stilton:   0%|          | 0/48 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d744654ee85748f2acda2274ad858d6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20555451250054715, 1: 19.143951744566863, 2: 22.60451624967967, 3: 73.66753492991445, 4: 39.902347417840375}\n",
            "map_width: 5539, map_height: 2838, horizontal_overlap: 26.304347826086957, vertical_overlap: 21.272727272727273\n",
            "Labelling Water_Orton.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 104 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Water_Orton:   0%|          | 0/288 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ab9ccde429d4d4e8c09a9ba7f879746"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.2050468145783027, 1: 19.588662620696827, 2: 26.56568303185896, 3: 79.61987041036717, 4: 45.84470330606162}\n",
            "map_width: 782, map_height: 626, horizontal_overlap: 80.66666666666667, vertical_overlap: 71.0\n",
            "Labelling Bath.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Bath:   0%|          | 0/12 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5234ead62b1a4ea49293547df1c56b96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20571335341283437, 1: 19.81577754963244, 2: 18.46444670187359, 3: 79.73342319625364, 4: 46.0785043465414}\n",
            "map_width: 1661, map_height: 1200, horizontal_overlap: 21.833333333333332, vertical_overlap: 20.0\n",
            "Labelling Swindon.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 32 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 37 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 38 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 39 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 41 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 46 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 47 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 54 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 55 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 58 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 59 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 78 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': ()} at index 94 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Swindon:   0%|          | 0/35 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "accc617c4b81439f971d1b2bfd3ff8f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20595971307942365, 1: 20.39796394180636, 2: 16.041541988894657, 3: 82.39881938715038, 4: 47.208154080701966}\n",
            "map_width: 1736, map_height: 1109, horizontal_overlap: 44.57142857142857, vertical_overlap: 42.75\n",
            "Labelling Northampton.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Northampton:   0%|          | 0/40 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3c18ed1ca8a499c842954cd3b34e3a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20701579958423355, 1: 20.9877415414124, 2: 11.163980787931651, 3: 84.83967186256072, 4: 48.91592182115725}\n",
            "map_width: 4718, map_height: 2461, horizontal_overlap: 21.157894736842106, vertical_overlap: 35.5\n",
            "Labelling Papworth.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Papworth:   0%|          | 0/220 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7a1e7c584df4c26bc9815e1e3deb4fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20664626581755025, 1: 18.5733196768914, 2: 13.27456636580433, 3: 79.22933174783346, 4: 52.580822048844645}\n",
            "map_width: 1116, map_height: 1238, horizontal_overlap: 41.0, vertical_overlap: 59.6\n",
            "Labelling Nottingham.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Nottingham:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfb92f8574c64049b703cdf56399ebb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20769972786176044, 1: 18.988520563856184, 2: 9.953693029290287, 3: 73.3886377270557, 4: 53.75625026156099}\n",
            "map_width: 1572, map_height: 1398, horizontal_overlap: 36.666666666666664, vertical_overlap: 27.6\n",
            "map_width: 1131, map_height: 832, horizontal_overlap: 37.25, vertical_overlap: 64.0\n",
            "Labelling Salisbury.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Salisbury:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8917c7b178e4429b431b9f17fb50ac1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.2081404096605086, 1: 19.265321155166042, 2: 8.979562283049875, 3: 72.99928541992583, 4: 53.817222481136916}\n",
            "map_width: 1160, map_height: 680, horizontal_overlap: 30.0, vertical_overlap: 44.0\n",
            "Labelling York.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing York:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f57801fc3114a6db9f071d166bc8774"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20866084903886875, 1: 19.472921598648433, 2: 8.04863195565836, 3: 73.78591565074399, 4: 54.39714944752848}\n",
            "map_width: 4168, map_height: 2025, horizontal_overlap: 25.88235294117647, vertical_overlap: 34.875\n",
            "Labelling Belsay.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Belsay:   0%|          | 0/162 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4152f846ecc8497fa4342a523a70cb26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.2085496612684951, 1: 18.547272138593197, 2: 8.573769062018211, 3: 66.39781743277145, 4: 51.63372617941701}\n",
            "map_width: 937, map_height: 914, horizontal_overlap: 29.0, vertical_overlap: 36.666666666666664\n",
            "Labelling Ripon.\n",
            "Error during polygon transformation: An input LineString must be valid.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasterio/features.py:328: ShapeSkipWarning: Invalid or empty shape None at index 11 will not be rasterized.\n",
            "  warnings.warn('Invalid or empty shape {} at index {} will not be rasterized.'.format(geom, index), ShapeSkipWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Ripon:   0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cf70eb555eb47afba98f7363192d911"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Weights: {0: 0.20887414958955328, 1: 18.736409394308616, 2: 8.051957535094475, 3: 64.33833280169463, 4: 51.7601309518174}\n"
          ]
        }
      ],
      "source": [
        "#@title Generate Training Data\n",
        "skeletonize_labels = False # @param {type:\"boolean\"}\n",
        "skeleton_buffer = 0 # @param {type:\"integer\"}\n",
        "\n",
        "import importlib\n",
        "\n",
        "# Check if Rasterio is installed\n",
        "try:\n",
        "    import rasterio\n",
        "except ImportError:\n",
        "    !pip install rasterio\n",
        "    import rasterio\n",
        "\n",
        "from osgeo import gdal, ogr, osr\n",
        "from shapely.geometry import mapping, box, shape, LineString, MultiLineString, Polygon, MultiPolygon, LinearRing\n",
        "from shapely.errors import TopologicalError\n",
        "\n",
        "import geopandas as gpd\n",
        "from affine import Affine\n",
        "import rasterio.features\n",
        "import rasterio.enums\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "import sys\n",
        "from skimage.transform import rotate\n",
        "from numpy import fliplr\n",
        "import shutil\n",
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "from matplotlib.colors import ListedColormap\n",
        "from io import BytesIO\n",
        "from skimage.morphology import skeletonize\n",
        "from scipy.ndimage import binary_dilation\n",
        "\n",
        "def plot_raster_with_classes(image, title=\"Labels\"):\n",
        "    # Get unique classes from the image\n",
        "    unique_classes = np.unique(image)\n",
        "\n",
        "    # Create a colormap for each class\n",
        "    cmap = plt.cm.get_cmap('viridis', len(unique_classes))\n",
        "\n",
        "    # Plot the image with different colors for each class\n",
        "    plt.imshow(image, cmap=cmap, interpolation='none')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(ticks=unique_classes)\n",
        "    plt.show()\n",
        "\n",
        "class_counts = np.zeros(num_classes, dtype=int) # Tally for class weights, updated by `split_map`\n",
        "class_weights = {}\n",
        "\n",
        "def calculate_overlaps(map, tile_size, min_overlap):\n",
        "    map_width, map_height = map.RasterXSize, map.RasterYSize\n",
        "\n",
        "    horizontal_count = math.ceil((map_width - min_overlap) / (tile_size - min_overlap))\n",
        "    vertical_count = math.ceil((map_height - min_overlap) / (tile_size - min_overlap))\n",
        "\n",
        "    horizontal_overlap = (tile_size * horizontal_count - map_width) / (horizontal_count - 1)\n",
        "    vertical_overlap = (tile_size * vertical_count - map_height) / (vertical_count - 1)\n",
        "\n",
        "    return horizontal_count, horizontal_overlap, vertical_count, vertical_overlap\n",
        "\n",
        "def transform_coordinates_to_image(geometry, transform):\n",
        "    if type(geometry) in (LineString, MultiLineString):\n",
        "        return transform_line_coordinates(geometry, transform)\n",
        "    elif type(geometry) in (Polygon, MultiPolygon):\n",
        "        return transform_polygon_coordinates(geometry, transform)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported geometry type\")\n",
        "\n",
        "def transform_line_coordinates(line, transform):\n",
        "    transformed_coords = [\n",
        "        (\n",
        "            round((coord[0] - transform[0]) / transform[1]),\n",
        "            round((coord[1] - transform[3]) / transform[5])\n",
        "        )\n",
        "        for coord in line.coords\n",
        "    ]\n",
        "    return LineString(transformed_coords)\n",
        "\n",
        "def transform_polygon_coordinates(polygon, transform):\n",
        "    try:\n",
        "        exterior = polygon.exterior\n",
        "        transformed_exterior = transform_line_coordinates(exterior, transform)\n",
        "\n",
        "        interiors = []\n",
        "        for interior in polygon.interiors:\n",
        "            transformed_interior = transform_line_coordinates(interior, transform)\n",
        "            # Reverse the order of coordinates in the interior ring\n",
        "            transformed_interior = LinearRing(transformed_interior.coords[::-1])\n",
        "            interiors.append(transformed_interior)\n",
        "\n",
        "        return Polygon(transformed_exterior, interiors)\n",
        "\n",
        "    except TopologicalError:\n",
        "        # Retry with an alternative approach\n",
        "        try:\n",
        "            exterior = polygon.exterior\n",
        "            transformed_exterior = transform_line_coordinates(exterior, transform)\n",
        "\n",
        "            interiors = []\n",
        "            for interior in polygon.interiors:\n",
        "                transformed_interior = transform_line_coordinates(interior, transform)\n",
        "                interiors.append(transformed_interior)\n",
        "\n",
        "            return Polygon(transformed_exterior, interiors)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error during polygon transformation:\", e)\n",
        "            return None  # Handle the error as needed\n",
        "\n",
        "\n",
        "def split_map(map_path, extent, tile_directory, tile_size, min_overlap, region_name, annotated, add_buffer, include_nolabels=True):\n",
        "\n",
        "    global class_counts\n",
        "\n",
        "    buffer_widths = { # Default pixels drawn on both sides of lines\n",
        "        1: 4,\n",
        "        2: 4,\n",
        "        3: 2,\n",
        "        4: 2,\n",
        "    }\n",
        "    padding_to_buffer = .8\n",
        "\n",
        "    map = gdal.Open(map_path)\n",
        "    map_width, map_height = map.RasterXSize, map.RasterYSize\n",
        "    horizontal_count, horizontal_overlap, vertical_count, vertical_overlap = calculate_overlaps(map, tile_size, min_overlap)\n",
        "\n",
        "    print(f\"map_width: {map_width}, map_height: {map_height}, horizontal_overlap: {horizontal_overlap}, vertical_overlap: {vertical_overlap}\")\n",
        "\n",
        "    def truecrop(gdf, extent, type):\n",
        "\n",
        "        cropped_geometries = []\n",
        "\n",
        "        for index, row in gdf.iterrows():\n",
        "            geom = row['geometry']\n",
        "            if geom.is_empty:\n",
        "                continue\n",
        "\n",
        "            # Use the extent_geometry to crop each part of the geometry\n",
        "            cropped_part = gpd.clip(gpd.GeoDataFrame(geometry=[geom]), box(*extent))\n",
        "            cropped_part = cropped_part[~cropped_part.is_empty]\n",
        "\n",
        "            if not cropped_part.empty:\n",
        "                row['geometry'] = cropped_part.iloc[0]['geometry']  # Update the 'geometry' column\n",
        "                cropped_geometries.append(row)\n",
        "\n",
        "        # Create a new GeoDataFrame with the modified rows\n",
        "        if not cropped_geometries:\n",
        "            return None\n",
        "        new_gdf = gpd.GeoDataFrame(cropped_geometries, geometry='geometry')\n",
        "\n",
        "        return new_gdf\n",
        "\n",
        "    if annotated is True:\n",
        "\n",
        "        label_raster_filepath = f\"{labels_raster_directory}{region_name}.label.npy\"\n",
        "        print(f'Labelling {region_name}.')\n",
        "        # Open the GeoPackage\n",
        "        wideroads_gdf = truecrop( gpd.read_file(geopackage_path, layer='wideroads'), extent, type='polygon' )\n",
        "        labels_gdf = truecrop( gpd.read_file(geopackage_path, layer='labels'), extent, type='linestring' )\n",
        "\n",
        "        transform = map.GetGeoTransform()  # Get the geotransformation matrix\n",
        "\n",
        "        # Collect shapes\n",
        "        shapes = []\n",
        "\n",
        "        if wideroads_gdf is not None:\n",
        "            for index, row in wideroads_gdf.iterrows():\n",
        "                row['geometry'] = transform_coordinates_to_image(row['geometry'], transform)\n",
        "                shapes.append((row['geometry'], row['class']))\n",
        "\n",
        "        if labels_gdf is not None:\n",
        "            for index, row in labels_gdf.iterrows():\n",
        "                row['geometry'] = transform_coordinates_to_image(row['geometry'], transform)\n",
        "                if not skeletonize_labels:\n",
        "                    if add_buffer is True:\n",
        "                        buffer = math.floor(row['padding'] * padding_to_buffer) if row['padding'].is_integer() else buffer_widths.get(row['type'], 0)\n",
        "                        row['geometry'] = row['geometry'].buffer(buffer)\n",
        "                    elif isinstance(add_buffer, int) and add_buffer > 0:\n",
        "                        row['geometry'] = row['geometry'].buffer(add_buffer)\n",
        "                shapes.append((row['geometry'], row['type']))\n",
        "\n",
        "        # Sort the shapes in descending order of 'type'/'class' value (primary roads drawn last)\n",
        "        shapes.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Use rasterio.features.rasterize with the sorted shapes list\n",
        "        label_image = rasterio.features.rasterize(\n",
        "            shapes=shapes,\n",
        "            out_shape=(map_height, map_width),\n",
        "            fill=0,\n",
        "            all_touched=True,\n",
        "            merge_alg=rasterio.enums.MergeAlg.replace,\n",
        "            dtype=np.uint8\n",
        "        )\n",
        "\n",
        "        # Skeletonize the label image using binary mask\n",
        "        if skeletonize_labels:\n",
        "            binary_label_image = (label_image > 0).astype(np.uint8)\n",
        "            skeletonized_binary_label_image = skeletonize(binary_label_image)\n",
        "            if skeleton_buffer > 0:\n",
        "                skeletonized_binary_label_image = binary_dilation(skeletonized_binary_label_image, iterations=skeleton_buffer) # Thicken the lines\n",
        "            label_image = skeletonized_binary_label_image * label_image\n",
        "\n",
        "        def save_map_with_labels(rgb_image, label_image, extent, region_name):\n",
        "\n",
        "            # Create a PIL Image from the rgb_image\n",
        "            img = Image.fromarray(rgb_image)\n",
        "\n",
        "            # Create an ImageDraw object\n",
        "            draw = ImageDraw.Draw(img)\n",
        "\n",
        "            # Draw the overlay using the specified colors for each class\n",
        "            for class_label, color in class_colors.items():\n",
        "                if class_label == 0:\n",
        "                    continue  # Skip the background class\n",
        "                overlay_pixels = (label_image == class_label)\n",
        "                for i in range(img.width):\n",
        "                    for j in range(img.height):\n",
        "                        if overlay_pixels[j, i]:\n",
        "                            draw.point((i, j), fill=color)\n",
        "\n",
        "            # Save the resulting image\n",
        "            overlay_path = f'{labels_overlay_directory}{region_name}.png'\n",
        "            img.save(overlay_path, 'PNG')\n",
        "\n",
        "        # Read the RGB image data\n",
        "        rgb_image_data = map.ReadAsArray().transpose(1, 2, 0)\n",
        "\n",
        "        # Plot the map with labels\n",
        "        save_map_with_labels(rgb_image_data, label_image, extent, region_name)\n",
        "\n",
        "        label_image = np.eye(num_classes, dtype=bool)[label_image] # One-hot encode the label image\n",
        "        np.save(label_raster_filepath, label_image)\n",
        "\n",
        "        # Load preprocessed map image\n",
        "        preprocessed = np.load(f\"{map_augmented_s1_directory}{region_name}.augmented_s1.npy\")\n",
        "\n",
        "        # Initialize the tqdm progress bar\n",
        "        total_iterations = horizontal_count * vertical_count\n",
        "        progress_bar = tqdm(total=total_iterations, desc=f\"Processing {region_name}\")\n",
        "\n",
        "        for x_loop in range(0, horizontal_count):\n",
        "            for y_loop in range(0, vertical_count):\n",
        "\n",
        "                x = round(x_loop * (tile_size - horizontal_overlap))\n",
        "                y = round(y_loop * (tile_size - vertical_overlap))\n",
        "\n",
        "                # Create a road image for the current tile\n",
        "                label_tile = label_image[y:y + tile_size, x:x + tile_size]\n",
        "\n",
        "                if include_nolabels or np.any(label_tile[..., 0] == 0): # Skip tiles without road labelling\n",
        "\n",
        "                    # Update class weight tally\n",
        "                    class_indices = np.argmax(label_tile, axis=-1)\n",
        "                    class_indices = class_indices.flatten()\n",
        "                    class_counts += np.bincount(class_indices, minlength=num_classes)\n",
        "\n",
        "                    label_tile_path = f\"{tile_directory}{region_name}_{x}_{y}.label.npy\"\n",
        "                    np.save(label_tile_path, label_tile)\n",
        "                    # Create a preprocessed map image for the current tile\n",
        "                    preprocessed_tile = preprocessed[y:y + tile_size, x:x + tile_size]\n",
        "                    preprocessed_tile_path = f\"{tile_directory}{region_name}_{x}_{y}.augmented_s1.npy\"\n",
        "                    np.save(preprocessed_tile_path, preprocessed_tile)\n",
        "\n",
        "                progress_bar.update(1)\n",
        "\n",
        "        # Ensure the progress bar reaches 100%\n",
        "        progress_bar.update(total_iterations - progress_bar.n)\n",
        "        progress_bar.close()\n",
        "\n",
        "        # Calculate class weights\n",
        "        total_samples = sum(class_counts)\n",
        "        for class_idx, count in enumerate(class_counts):\n",
        "            if count > 0:\n",
        "                class_weight = total_samples / (num_classes * count)\n",
        "                class_weights[class_idx] = class_weight\n",
        "\n",
        "        print(f\"Combined Class Weights: {class_weights}\")\n",
        "        with open(class_weights_file, 'w') as json_file:\n",
        "            json.dump(class_weights, json_file)\n",
        "\n",
        "# Clear any pre-existing tiles\n",
        "if os.path.exists(tile_directory):\n",
        "    shutil.rmtree(tile_directory)\n",
        "os.makedirs(tile_directory)\n",
        "\n",
        "# Open the GeoPackage\n",
        "regions_gdf = gpd.read_file(geopackage_path, layer='regions')\n",
        "\n",
        "# Loop through each region in the GeoDataFrame\n",
        "for index, row in regions_gdf.iterrows():\n",
        "    # Extract the region name, URL, and other attributes\n",
        "    region_name = row['name']\n",
        "    annotated = row['annotated']\n",
        "    geom = row['geometry']\n",
        "\n",
        "    # Get the extent (bounding box) of the geometry\n",
        "    extent = geom.bounds\n",
        "\n",
        "    map_filename = f\"{region_name}.jpg\"\n",
        "    map_path = os.path.join(map_directory, map_filename)\n",
        "\n",
        "    # Carve map and labels into square chips\n",
        "    min_overlap = 16 # Minimum tile overlap (px)\n",
        "    split_map(map_path, extent, tile_directory, tile_size, min_overlap, region_name, annotated, add_buffer=True, include_nolabels=True)\n",
        "\n",
        "    # if index==0:\n",
        "    #     sys.exit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jtTc68qhJC7L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "3dc948ded1fb48ff9209a5ea1996ce79",
            "6fb7aae4e5144e91b283c47a18b56c6d",
            "f9175a82bafb4425b73f44be2c3db19a",
            "227afb3a245f4b59b2be6d634250cda3",
            "ec24c11bb2f64c3184c7c44807217255",
            "5e96d07a1b4a4926a02b18819640b15b",
            "3103ac59bf96464497ebc9ad6e2905ac",
            "66e71205ee124ecda6bc9e799a2b2c0e",
            "dc3b2d9594024f02b08f037f84bb1fe0",
            "5062f9f6330c42248d45e67a24955df8",
            "72db425d924a4d06aef45866863dc4ab",
            "d0f73b0c7a85475fa14e6edf0babe92a",
            "5062e53f83304908ae035533d3957169",
            "53d056905dce43398f97494e2c9c62ea",
            "24045185ac5d4351b8e17cd842e65af9",
            "516a5aaee924406fa150a8473bf914e4",
            "55b387b5b36647e2a3c2b3eb660a4259",
            "d1f5893620b84996bc3369b9e4c3f72b",
            "5e65c7ef4f2742ccaecafc9a6bb494cb",
            "2a1b51af19d846cfb81411954163ff1e",
            "a57caf52fa0a47f3b5e2bc8ae78df256",
            "8f5d08b3d9a041058bce08738a06ea8c",
            "57cac508dc594cca8ac6aaf941978b25",
            "f8413af1b058453ca3b57c6dc5b86eb4",
            "c25d462dfe1c41478e8ad06a21de5abe",
            "31e4c56563034a60abd42cd84ea9d4f1",
            "c7e1a60a2181454c932f685a73e63a59",
            "5c4a109975af4d3da01607397928561e",
            "7ba2daf471b74edb93e4532e07abd90c",
            "551e59febce14f42b075d1ec14a340b1",
            "b6277b8f7f0347c39898ceafeb81dbb1",
            "9522edca08ad405889adc8a839a35f9a",
            "0f5a9ce9e10d496fa97b88a35ec84b16",
            "8963cd24e7074e0694c7804d58abd1eb",
            "0457973170e94752843fd829a1ed2c28",
            "48fad2f7fb174d57a8d85a264121e0ec",
            "686fd2757e6744ee9a84403918ac08c3",
            "3001754faab045e8b4d71ed723ff1d1c",
            "bd81608e5c044028a9ea0409267938cb",
            "c7ab6ae0a275403aaa216130734e6909",
            "4348c7735acf426fa515efeeef9f6d56",
            "fd013ab70e9349959ea8956776e7b90e",
            "ac68c256941e40f49f8ac42945ae6405",
            "42153fa1093d4e77b16da01b52a24850"
          ]
        },
        "id": "KYGCmiIOMzqn",
        "outputId": "5822ae3c-2ffb-497b-e8e7-ddcad18e374f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of label files in the dataset folder: 1585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 85 TFRecords for training with 1347 data pairs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Data Augmentation:   0%|          | 0/1347 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dc948ded1fb48ff9209a5ea1996ce79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating TFRecords for training_data/train:   0%|          | 0/421 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0f73b0c7a85475fa14e6edf0babe92a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 15 TFRecords for evaluation with 238 data pairs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Data Augmentation:   0%|          | 0/238 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57cac508dc594cca8ac6aaf941978b25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating TFRecords for training_data/eval:   0%|          | 0/75 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8963cd24e7074e0694c7804d58abd1eb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Create TFRecords and copy to GCS Bucket\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import fnmatch\n",
        "import tensorflow as tf\n",
        "from google.cloud import storage\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from skimage.transform import rotate\n",
        "from numpy import fliplr\n",
        "\n",
        "augment_training_data = True # @param {type:\"boolean\"}\n",
        "\n",
        "def augment(tilepath, flip=True):\n",
        "    tile = np.load(tilepath)\n",
        "    # rotate and save three times, then flip and save, then rotate and save three times\n",
        "    augmented_files = []\n",
        "    for rotation in range(3):\n",
        "        rotated_tile = rotate(tile, 90 * (rotation + 1), resize=False, preserve_range=True).astype(np.uint8)\n",
        "        rotated_path = tilepath.replace('.', f\"_r{rotation + 1}.\", 1)\n",
        "        np.save(rotated_path, rotated_tile)\n",
        "        augmented_files.append(os.path.basename(rotated_path))\n",
        "    if flip is True:\n",
        "        flipped_tile = fliplr(tile)\n",
        "        flipped_path = tilepath.replace('.', '_f.', 1)\n",
        "        np.save(flipped_path, flipped_tile)\n",
        "        augmented_files.append(flipped_path)\n",
        "        augment(flipped_path, False)\n",
        "\n",
        "    return augmented_files\n",
        "\n",
        "# Function to create a directory if it doesn't exist or clear it if it does\n",
        "def create_or_clear_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        shutil.rmtree(directory)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Count and shuffle label files\n",
        "label_files = [f for f in os.listdir(tile_directory) if fnmatch.fnmatch(f, '*.label.npy')]\n",
        "random.shuffle(label_files)\n",
        "dataset_count = len(label_files)\n",
        "print(f\"Number of label files in the dataset folder: {dataset_count}\")\n",
        "\n",
        "# Calculate split sizes\n",
        "evalsplit = math.ceil(dataset_count * eval_ratio)\n",
        "trainsplit = dataset_count - evalsplit\n",
        "\n",
        "# Initiate Google Cloud bucket\n",
        "client = storage.Client(project=gcs_project_id)\n",
        "gcs_bucket = client.get_bucket(gcs_bucket_name)\n",
        "\n",
        "# Delete pre-existing files in the GCS bucket\n",
        "blobs = gcs_bucket.list_blobs(prefix=gcs_data_directory)\n",
        "for blob in blobs:\n",
        "    blob.delete()\n",
        "\n",
        "def create_tfrecord_files(files, output_dir, bucket, gcs_dir):\n",
        "\n",
        "    # Apply data augmentation\n",
        "    if augment_training_data:\n",
        "        augmented_files = []\n",
        "        for file in tqdm(files, desc=\"Data Augmentation\"):\n",
        "            label_path = os.path.join(tile_directory, file)\n",
        "            image_path = label_path.replace(\".label.npy\", \".augmented_s1.npy\")\n",
        "            augment(image_path) # Do not add to the file list, which is of labels only\n",
        "            augmented_files.extend(augment(label_path))\n",
        "        files.extend(augmented_files)\n",
        "        random.shuffle(files)\n",
        "\n",
        "    create_or_clear_directory(output_dir)\n",
        "\n",
        "    for i in tqdm(range(0, len(files), TFRecord_batch_size), desc=f\"Creating TFRecords for {gcs_dir}\"):\n",
        "\n",
        "        if i + TFRecord_batch_size > len(files):\n",
        "            batch_files = files[i:]\n",
        "        else:\n",
        "            batch_files = files[i:i + TFRecord_batch_size]\n",
        "\n",
        "        batch_records = []\n",
        "        for file in batch_files:\n",
        "            label_path = os.path.join(tile_directory, file)\n",
        "            image_path = label_path.replace(\".label.npy\", \".augmented_s1.npy\")\n",
        "\n",
        "            # Read image and label data\n",
        "            label_data = np.load(label_path).astype(np.uint8)\n",
        "            image_data = np.load(image_path).astype(np.uint8)\n",
        "\n",
        "            # Create a TFRecord\n",
        "            example = tf.train.Example(features=tf.train.Features(\n",
        "                feature={\n",
        "                    'label': tf.train.Feature(\n",
        "                        bytes_list=tf.train.BytesList(value=[label_data.tobytes()])\n",
        "                    ),\n",
        "                    'image': tf.train.Feature(\n",
        "                        bytes_list=tf.train.BytesList(value=[image_data.tobytes()])\n",
        "                    )\n",
        "                }\n",
        "            ))\n",
        "            batch_records.append(example.SerializeToString())\n",
        "\n",
        "        # Write the batch of TFRecords to the output directory, then copy to the GCS bucket\n",
        "        tfrecord_filename = f\"batch_{i}.tfrecord\"\n",
        "        tfrecord_filepath = os.path.join(output_dir, tfrecord_filename)\n",
        "        with tf.io.TFRecordWriter(tfrecord_filepath) as writer:\n",
        "            for record in batch_records:\n",
        "                writer.write(record)\n",
        "        blob = bucket.blob(os.path.join(gcs_dir, tfrecord_filename))\n",
        "        blob.upload_from_filename(tfrecord_filepath)\n",
        "\n",
        "\n",
        "\n",
        "# Create TFRecord files in the GCS bucket\n",
        "\n",
        "print(f\"Creating {math.ceil(trainsplit/TFRecord_batch_size)} TFRecords for training with {trainsplit} data pairs.\")\n",
        "create_tfrecord_files(label_files[:trainsplit], f\"{tile_directory}/train\", gcs_bucket, f\"{gcs_data_directory}/train\")\n",
        "label_files[:trainsplit] = []\n",
        "\n",
        "print(f\"Creating {math.ceil(evalsplit/TFRecord_batch_size)} TFRecords for evaluation with {evalsplit} data pairs.\")\n",
        "create_tfrecord_files(label_files, f\"{tile_directory}/eval\", gcs_bucket, f\"{gcs_data_directory}/eval\")\n"
      ]
    }
  ]
}