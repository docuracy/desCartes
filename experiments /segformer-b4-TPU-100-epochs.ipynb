{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docuracy/desCartes/blob/main/experiments%20/segformer-b4-TPU-100-epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Authenticate GCS, WandB, and Hugging Face; mount Google Drive; install dependencies\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!gcloud auth application-default login\n",
        "!gcloud config set project descartes-404713\n",
        "\n",
        "!pip install wandb -qU\n",
        "!wandb login\n",
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "!pip install opencv-python\n",
        "!pip install --upgrade torch_xla torch\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "Xx4m0dgRcKNe"
      },
      "id": "Xx4m0dgRcKNe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Data from GCS { display-mode: \"code\" }\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from google.cloud import storage\n",
        "from transformers import SegformerImageProcessor\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Google Drive Path Configuration\n",
        "project_path = '/content/drive/MyDrive/desCartes'\n",
        "\n",
        "# Google Cloud Storage (GCS) configuration\n",
        "gcs_key_path = f'{project_path}/descartes-404713-cccf7c3921aa.json'\n",
        "gcs_project_id = 'descartes-404713'\n",
        "gcs_bucket_name = 'descartes'\n",
        "gcs_data_directory = \"training_data\"\n",
        "\n",
        "# Authenticate with your GCS key file\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcs_key_path\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Initialize image processor (same as before)\n",
        "image_processor = SegformerImageProcessor.from_pretrained('nvidia/segformer-b2-finetuned-ade-512-512')\n",
        "\n",
        "# Function to check if a .pt file is loadable\n",
        "def check_loadable(file_data):\n",
        "    try:\n",
        "        # Attempt to load the tensor from the file data\n",
        "        data = torch.load(io.BytesIO(file_data))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load dataset from GCS into memory\n",
        "def load_data_from_gcs(bucket_name, data_directory):\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=data_directory)  # List all blobs in the data directory\n",
        "\n",
        "    data = []  # This will hold the loaded data\n",
        "\n",
        "    for blob in blobs:\n",
        "        if blob.name.endswith(\".pt\"):\n",
        "            print(f\"Processing {blob.name}...\")\n",
        "\n",
        "            # Read the blob into memory (without saving it locally)\n",
        "            file_data = blob.download_as_bytes()\n",
        "\n",
        "            # Check if the file is loadable\n",
        "            if check_loadable(file_data):\n",
        "                try:\n",
        "                    # Load data directly into memory\n",
        "                    file_tensor = torch.load(io.BytesIO(file_data))\n",
        "                    inputs = image_processor(images=file_tensor['images'], return_tensors=\"pt\")\n",
        "                    pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "                    label = file_tensor['labels'].squeeze().long()\n",
        "\n",
        "                    # Append the data to the list\n",
        "                    data.append({\"pixel_values\": pixel_values, \"labels\": label})\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {blob.name}: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping corrupt file: {blob.name}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load train and eval data from GCS\n",
        "train_data = load_data_from_gcs(gcs_bucket_name, f\"{gcs_data_directory}/train\")\n",
        "eval_data = load_data_from_gcs(gcs_bucket_name, f\"{gcs_data_directory}/eval\")\n",
        "\n",
        "# Convert the loaded data into a custom dataset\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create datasets for training and evaluation\n",
        "train_dataset = SegmentationDataset(train_data)\n",
        "eval_dataset = SegmentationDataset(eval_data)\n",
        "\n",
        "# Now train_dataset and eval_dataset are ready to be used for training\n"
      ],
      "metadata": {
        "id": "fdJhv02p5nCA"
      },
      "id": "fdJhv02p5nCA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save SegmentationDatasets to Drive { display-mode: \"code\" }\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Save the dataset (train and eval data) to a binary file\n",
        "def save_dataset(dataset, file_path):\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "    torch.save(dataset, file_path)\n",
        "    print(f\"Dataset saved to {file_path}\")\n",
        "\n",
        "# Define file paths for saving\n",
        "train_data_path = '/content/drive/MyDrive/desCartes/pytorch/train_data.pt'\n",
        "eval_data_path = '/content/drive/MyDrive/desCartes/pytorch/eval_data.pt'\n",
        "\n",
        "# Save the datasets\n",
        "save_dataset(train_dataset, train_data_path)\n",
        "save_dataset(eval_dataset, eval_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPXcAwIrAK5m",
        "outputId": "c2b4e3ea-c0d3-4dbe-ece8-85730a38bc72"
      },
      "id": "bPXcAwIrAK5m",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to /content/drive/MyDrive/desCartes/pytorch/train_data.pt\n",
            "Dataset saved to /content/drive/MyDrive/desCartes/pytorch/eval_data.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load SegmentationDatasets from Drive { display-mode: \"code\" }\n",
        "\n",
        "import torch\n",
        "\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "torch.serialization.add_safe_globals([SegmentationDataset])\n",
        "\n",
        "# Load the dataset from a binary file\n",
        "def load_dataset(file_path):\n",
        "    dataset = torch.load(file_path)\n",
        "    print(f\"Dataset loaded from {file_path}\")\n",
        "    return dataset\n",
        "\n",
        "# Define file paths for loading\n",
        "train_data_path = '/content/drive/MyDrive/desCartes/pytorch/train_data.pt'\n",
        "eval_data_path = '/content/drive/MyDrive/desCartes/pytorch/eval_data.pt'\n",
        "\n",
        "# Load the datasets from Google Drive\n",
        "eval_dataset = load_dataset(eval_data_path)\n",
        "train_dataset = load_dataset(train_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLmEZ9AIBh2G",
        "outputId": "0b1d4d71-6daa-4e1f-ca5f-5999d1168c4c"
      },
      "id": "kLmEZ9AIBh2G",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded from /content/drive/MyDrive/desCartes/pytorch/eval_data.pt\n",
            "Dataset loaded from /content/drive/MyDrive/desCartes/pytorch/train_data.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Safe Mean IoU { display-mode: \"code\" }\n",
        "\n",
        "# Copyright 2022 The HuggingFace Evaluate Authors.\n",
        "# Based on https://huggingface.co/spaces/evaluate-metric/mean_iou/blob/main/mean_iou.py\n",
        "\"\"\"Mean IoU (Intersection-over-Union) metric.\"\"\"\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "\n",
        "import evaluate\n",
        "\n",
        "def intersect_and_union(\n",
        "    pred_label,\n",
        "    label,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    if label_map is not None:\n",
        "        for old_id, new_id in label_map.items():\n",
        "            label[label == old_id] = new_id\n",
        "\n",
        "    # turn into Numpy arrays\n",
        "    pred_label = np.array(pred_label)\n",
        "    label = np.array(label)\n",
        "\n",
        "    if reduce_labels:\n",
        "        label[label == 0] = 255\n",
        "        label = label - 1\n",
        "        label[label == 254] = 255\n",
        "\n",
        "    mask = label != ignore_index\n",
        "    mask = np.not_equal(label, ignore_index)\n",
        "    pred_label = pred_label[mask]\n",
        "    label = np.array(label)[mask]\n",
        "\n",
        "    intersect = pred_label[pred_label == label]\n",
        "\n",
        "    area_intersect = np.histogram(intersect, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "    area_pred_label = np.histogram(pred_label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "    area_label = np.histogram(label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "\n",
        "    area_union = area_pred_label + area_label - area_intersect\n",
        "\n",
        "    return area_intersect, area_union, area_pred_label, area_label\n",
        "\n",
        "\n",
        "def total_intersect_and_union(\n",
        "    results,\n",
        "    gt_seg_maps,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    total_area_intersect = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_union = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_pred_label = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_label = np.zeros((num_labels,), dtype=np.float64)\n",
        "    for result, gt_seg_map in zip(results, gt_seg_maps):\n",
        "        area_intersect, area_union, area_pred_label, area_label = intersect_and_union(\n",
        "            result, gt_seg_map, num_labels, ignore_index, label_map, reduce_labels\n",
        "        )\n",
        "        total_area_intersect += area_intersect\n",
        "        total_area_union += area_union\n",
        "        total_area_pred_label += area_pred_label\n",
        "        total_area_label += area_label\n",
        "    return total_area_intersect, total_area_union, total_area_pred_label, total_area_label\n",
        "\n",
        "\n",
        "def mean_iou(\n",
        "    results,\n",
        "    gt_seg_maps,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    nan_to_num: Optional[int] = None,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    total_area_intersect, total_area_union, total_area_pred_label, total_area_label = total_intersect_and_union(\n",
        "        results, gt_seg_maps, num_labels, ignore_index, label_map, reduce_labels\n",
        "    )\n",
        "\n",
        "    # compute metrics\n",
        "    metrics = dict()\n",
        "    eps = 1e-10  # Small constant to prevent division by zero\n",
        "    min_val, max_val = 1e-5, 1 - 1e-5  # Clip range to avoid extreme values\n",
        "    round_decimals = 5  # Number of decimal places for rounding\n",
        "\n",
        "    # Compute metrics with epsilon and clipping\n",
        "    all_acc = np.clip(total_area_intersect.sum() / (total_area_label.sum() + eps), min_val, max_val)\n",
        "    iou = np.clip(total_area_intersect / (total_area_union + eps), min_val, max_val)\n",
        "    acc = np.clip(total_area_intersect / (total_area_label + eps), min_val, max_val)\n",
        "\n",
        "    # Round values, ensuring that values like 9.9999e-01 are rounded up to 1.0\n",
        "    iou = np.round(iou, round_decimals)\n",
        "    acc = np.round(acc, round_decimals)\n",
        "\n",
        "    # Explicitly round values very close to 1.0 (e.g., 0.99999, 0.999999)\n",
        "    iou = np.where(iou >= 0.99999, 1.0, iou)\n",
        "    acc = np.where(acc >= 0.99999, 1.0, acc)\n",
        "\n",
        "    # Calculate final metrics with rounding\n",
        "    metrics = {\n",
        "        \"mean_iou\": round(np.nanmean(iou), round_decimals),\n",
        "        \"mean_accuracy\": round(np.nanmean(acc), round_decimals),\n",
        "        \"overall_accuracy\": round(all_acc, round_decimals),\n",
        "        \"per_category_iou\": iou,\n",
        "        \"per_category_accuracy\": acc,\n",
        "    }\n",
        "\n",
        "    if nan_to_num is not None:\n",
        "        metrics = dict(\n",
        "            {metric: np.nan_to_num(metric_value, nan=nan_to_num) for metric, metric_value in metrics.items()}\n",
        "        )\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
        "class MeanIoU(evaluate.Metric):\n",
        "    def _info(self):\n",
        "        return evaluate.MetricInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            citation=_CITATION,\n",
        "            inputs_description=_KWARGS_DESCRIPTION,\n",
        "            features=datasets.Features(\n",
        "                {\n",
        "                    \"predictions\": datasets.Image(),\n",
        "                    \"references\": datasets.Image(),\n",
        "                }\n",
        "            ),\n",
        "            reference_urls=[\n",
        "                \"https://github.com/open-mmlab/mmsegmentation/blob/71c201b1813267d78764f306a297ca717827c4bf/mmseg/core/evaluation/metrics.py\"\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    def _compute(\n",
        "        self,\n",
        "        predictions,\n",
        "        references,\n",
        "        num_labels: int,\n",
        "        ignore_index: bool,\n",
        "        nan_to_num: Optional[int] = None,\n",
        "        label_map: Optional[Dict[int, int]] = None,\n",
        "        reduce_labels: bool = False,\n",
        "    ):\n",
        "        iou_result = mean_iou(\n",
        "            results=predictions,\n",
        "            gt_seg_maps=references,\n",
        "            num_labels=num_labels,\n",
        "            ignore_index=ignore_index,\n",
        "            nan_to_num=nan_to_num,\n",
        "            label_map=label_map,\n",
        "            reduce_labels=reduce_labels,\n",
        "        )\n",
        "        return iou_result"
      ],
      "metadata": {
        "id": "E4cS_c8TNAup"
      },
      "id": "E4cS_c8TNAup",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the cell with the mean_iou function has been run.\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "predictions = [np.array([[1, 2], [3, 4]]), np.array([[2, 3], [4, 1]])]\n",
        "ground_truth = [np.array([[1, 1], [3, 4]]), np.array([[2, 3], [4, 2]])]\n",
        "num_labels = 5  # Example number of labels\n",
        "ignore_index = 255  # Example ignore index\n",
        "\n",
        "# Call the mean_iou function\n",
        "iou_results = mean_iou(\n",
        "    results=predictions,\n",
        "    gt_seg_maps=ground_truth,\n",
        "    num_labels=num_labels,\n",
        "    ignore_index=ignore_index,\n",
        "    reduce_labels=False,\n",
        ")\n",
        "\n",
        "# Print the results\n",
        "print(iou_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p0A1o73Te7z",
        "outputId": "d0bcf498-cc5c-4eaa-b94e-5539a394f923"
      },
      "id": "2p0A1o73Te7z",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_iou': np.float64(0.53333), 'mean_accuracy': np.float64(0.6), 'overall_accuracy': np.float64(0.75), 'per_category_iou': array([1.0000e-05, 3.3333e-01, 3.3333e-01, 1.0000e+00, 1.0000e+00]), 'per_category_accuracy': array([1.e-05, 5.e-01, 5.e-01, 1.e+00, 1.e+00])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAVtAKuebADW",
        "outputId": "767173ee-0076-425c-c9c2-3c28cc3ad746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0/8 using device xla:0\n"
          ]
        }
      ],
      "source": [
        "# @title Train Model { display-mode: \"code\" }\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import wandb\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.runtime as xr\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "# Tidy up output [ineffective for TPUs]\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", message=\"Some weights of SegformerForSemanticSegmentation were not initialized\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", message=\".*feature_extractor_type.*\", category=UserWarning)\n",
        "\n",
        "# Google Drive Path Configuration\n",
        "project_path = '/content/drive/MyDrive/desCartes'\n",
        "model_path = f'{project_path}/models'\n",
        "results_path = f'{project_path}/results'\n",
        "\n",
        "# Select Model\n",
        "model_version = 'b4'\n",
        "\n",
        "# Define class labels\n",
        "class_labels = [\"background\", \"main_road\", \"minor_road\", \"semi_enclosed_path\", \"unenclosed_path\"]\n",
        "\n",
        "# Local directory for storing dataset\n",
        "local_data_dir = \"/content/data\"\n",
        "\n",
        "# Training Configuration\n",
        "per_device_train_batch_size = 2  # Batch size for training\n",
        "per_device_eval_batch_size = per_device_train_batch_size\n",
        "gradient_accumulation_steps = 1  # Simulates a batch size of gradient_accumulation_steps * per_device_train_batch_size\n",
        "\n",
        "###################################################\n",
        "\n",
        "# Configure label mappings\n",
        "num_classes = len(class_labels)\n",
        "id2label = {i: label for i, label in enumerate(class_labels)}\n",
        "label2id = {label: i for i, label in id2label.items()}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            logits, labels = eval_pred\n",
        "            logits_tensor = torch.from_numpy(logits)\n",
        "\n",
        "            # Upsample logits to match labels\n",
        "            logits_tensor = nn.functional.interpolate(\n",
        "                logits_tensor,\n",
        "                size=labels.shape[-2:],  # Match height & width of labels\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            ).argmax(dim=1)  # Convert to predicted class indices\n",
        "\n",
        "            pred_labels = logits_tensor.detach().cpu().numpy()\n",
        "\n",
        "            # Call the safe mean_iou function (defined in another cell)\n",
        "            metrics = mean_iou(\n",
        "                results=pred_labels,\n",
        "                gt_seg_maps=labels,\n",
        "                num_labels=num_classes,\n",
        "                ignore_index=0, # class 0 is background/ignored\n",
        "                reduce_labels=False,\n",
        "            )\n",
        "\n",
        "            # Extract per-class IoU & accuracy\n",
        "            per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
        "            per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
        "\n",
        "            # Compute precision, recall, and F1-score (excluding background)\n",
        "            pred_flat = pred_labels.flatten()\n",
        "            labels_flat = labels.flatten()\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                labels_flat, pred_flat, average=\"weighted\", zero_division=0\n",
        "            )\n",
        "\n",
        "            # Store overall metrics\n",
        "            metrics[\"overall_accuracy\"] = metrics.pop(\"mean_accuracy\")\n",
        "            metrics[\"overall_mean_iou\"] = metrics.pop(\"mean_iou\")\n",
        "            metrics[\"precision\"] = precision\n",
        "            metrics[\"recall\"] = recall\n",
        "            metrics[\"f1_score\"] = f1\n",
        "\n",
        "            # Add per-class accuracy & IoU\n",
        "            metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
        "            metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return default zeroed metrics with an error flag\n",
        "        return {\n",
        "            \"error\": True,\n",
        "            \"overall_accuracy\": 0.0,\n",
        "            \"overall_mean_iou\": 0.0,\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"f1_score\": 0.0,\n",
        "            **{f\"accuracy_{id2label[i]}\": 0.0 for i in range(num_classes)},\n",
        "            **{f\"iou_{id2label[i]}\": 0.0 for i in range(num_classes)},\n",
        "        }\n",
        "\n",
        "def _mp_fn(rank):\n",
        "    # Set TPU device inside the function\n",
        "    device = xm.xla_device()\n",
        "    world_size = xr.world_size()\n",
        "    xm.master_print(f\"Process {rank}/{world_size} using device {device}\")\n",
        "\n",
        "    # Synchronize TPUs before starting\n",
        "    xm.rendezvous(\"start_training\")  # Ensure all TPU processes sync before proceeding\n",
        "\n",
        "    # Initialize WandB only for the main TPU process\n",
        "    if rank == 0:\n",
        "        wandb.init(project=\"tpu-segmentation\", name=f\"TPU-Training-{model_version}\")\n",
        "\n",
        "    # Load the image processor and model inside _mp_fn\n",
        "    image_processor = SegformerImageProcessor.from_pretrained(f'nvidia/segformer-{model_version}-finetuned-ade-512-512')\n",
        "\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        f\"nvidia/segformer-{model_version}-finetuned-ade-512-512\",\n",
        "        num_labels=num_classes,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    ).to(device)\n",
        "\n",
        "    # Distributed samplers (drop_last=True to prevent hanging)\n",
        "    train_sampler = DistributedSampler(\n",
        "        train_dataset, num_replicas=world_size, rank=rank, shuffle=True, drop_last=True\n",
        "    )\n",
        "    eval_sampler = DistributedSampler(\n",
        "        eval_dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    # Safe TPU DataLoader setup\n",
        "    def worker_init_fn(worker_id):\n",
        "        \"\"\"Ensures each worker has a different random seed\"\"\"\n",
        "        torch.manual_seed(worker_id + rank)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, batch_size=per_device_train_batch_size, sampler=train_sampler,\n",
        "        num_workers=4, pin_memory=True, persistent_workers=False, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, batch_size=per_device_eval_batch_size, sampler=eval_sampler,\n",
        "        num_workers=4, pin_memory=True, persistent_workers=False, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    # Wrap data loaders with MpDeviceLoader for TPU support\n",
        "    train_dataloader = pl.MpDeviceLoader(train_dataloader, device)\n",
        "    eval_dataloader = pl.MpDeviceLoader(eval_dataloader, device)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"{model_path}/checkpoints\",\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        dataloader_num_workers=4,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=5,  # Keep only the last 5 checkpoints\n",
        "        logging_steps=10,\n",
        "        logging_strategy=\"steps\",\n",
        "        report_to=[\"wandb\"] if rank == 0 else [],\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        num_train_epochs=100,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        metric_for_best_model=\"overall_mean_iou\",  # Metric to monitor for best model\n",
        "        greater_is_better=True,  # Set to True to maximize the metric\n",
        "        run_name=f\"desCartes-{model_version}-{per_device_train_batch_size}-{gradient_accumulation_steps}-bf16\"\n",
        "    )\n",
        "\n",
        "    # Trainer: override standard dataloader methods\n",
        "    class CustomTrainer(Trainer):\n",
        "        def get_train_dataloader(self):\n",
        "            return train_dataloader\n",
        "\n",
        "        def get_eval_dataloader(self, eval_dataset=None):\n",
        "            return eval_dataloader\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "    )\n",
        "\n",
        "    trainer.train(resume_from_checkpoint=os.path.exists(f\"{model_path}/checkpoints\"))\n",
        "    xm.rendezvous(\"training_complete\")  # Ensure all TPU processes sync before exit\n",
        "\n",
        "    if rank == 0:\n",
        "        wandb.finish()  # Close WandB properly [Leave open for metrics via API]\n",
        "\n",
        "\n",
        "# Launch TPU training\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    xmp.spawn(_mp_fn, args=(), start_method='fork')\n"
      ],
      "id": "LAVtAKuebADW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pluiD9jkbADY"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Results { display-mode: \"code\" }\n",
        "\n",
        "# Function to display images and predicted masks\n",
        "def plot_predictions(model, dataset, n_samples=3):\n",
        "    for i, (images, labels) in enumerate(dataset.take(n_samples)):\n",
        "        predictions = model(images).logits\n",
        "        predictions = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        for j in range(min(n_samples, len(images))):\n",
        "            image = images[j].numpy()\n",
        "            label = labels[j].numpy()\n",
        "            prediction = predictions[j].numpy()\n",
        "\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            axes[0].imshow(image)\n",
        "            axes[0].set_title('Input Image')\n",
        "            axes[1].imshow(np.argmax(label, axis=-1), cmap='viridis')\n",
        "            axes[1].set_title('True Label')\n",
        "            axes[2].imshow(prediction, cmap='viridis')\n",
        "            axes[2].set_title('Predicted Mask')\n",
        "            plt.show()\n",
        "\n",
        "# Display some predictions\n",
        "plot_predictions(model, val_dataset)\n"
      ],
      "id": "pluiD9jkbADY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fc-sohJbADY"
      },
      "outputs": [],
      "source": [
        "# @title Evaluation Metrics { display-mode: \"code\" }\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to calculate metrics for model evaluation\n",
        "def evaluate_model(model, dataset):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in dataset.take(10):  # evaluate on first 10 batches\n",
        "        predictions = model(images).logits\n",
        "        preds = tf.argmax(predictions, axis=-1).numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Flatten the lists for classification_report\n",
        "    all_preds = np.concatenate(all_preds).flatten()\n",
        "    all_labels = np.concatenate(all_labels).flatten()\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    return report\n",
        "\n",
        "# Print evaluation metrics\n",
        "eval_report = evaluate_model(model, val_dataset)\n",
        "print(\"Evaluation Metrics:\\n\", eval_report)\n"
      ],
      "id": "1fc-sohJbADY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oZaMefIbADY"
      },
      "outputs": [],
      "source": [
        "# @title Model Saving { display-mode: \"code\" }\n",
        "# Save the trained model\n",
        "model.save_pretrained(f'{model_path}/segformer_model')\n",
        "# Save the image processor\n",
        "image_processor.save_pretrained(f'{model_path}/image_processor')\n"
      ],
      "id": "2oZaMefIbADY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0beeaRj9bADY"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Training Logs { display-mode: \"code\" }\n",
        "import os\n",
        "\n",
        "# Function to plot training logs\n",
        "def plot_logs(log_dir='./logs'):\n",
        "    log_files = [f for f in os.listdir(log_dir) if f.endswith('.json')]\n",
        "\n",
        "    if len(log_files) == 0:\n",
        "        print(\"No log files found.\")\n",
        "        return\n",
        "\n",
        "    log_file = log_files[0]\n",
        "    log_path = os.path.join(log_dir, log_file)\n",
        "    logs = []\n",
        "\n",
        "    with open(log_path, 'r') as f:\n",
        "        logs = f.readlines()\n",
        "\n",
        "    steps, losses = [], []\n",
        "    for log in logs:\n",
        "        if 'step' in log and 'loss' in log:\n",
        "            step = int(log.split('step')[1].split(',')[0].strip())\n",
        "            loss = float(log.split('loss')[1].split(',')[0].strip())\n",
        "            steps.append(step)\n",
        "            losses.append(loss)\n",
        "\n",
        "    plt.plot(steps, losses)\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Progress')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training logs\n",
        "plot_logs()\n"
      ],
      "id": "0beeaRj9bADY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}