{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docuracy/desCartes/blob/main/experiments/segformer-b4-TPU-loss-prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive; install dependencies\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!pip install opencv-python\n",
        "!pip install --upgrade torch_xla torch\n",
        "!pip install evaluate\n",
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "Xx4m0dgRcKNe"
      },
      "id": "Xx4m0dgRcKNe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downgrade Package for Compatibility (required when continuing training if package has been updated)\n",
        "\n",
        "# !pip uninstall -y transformers\n",
        "# !pip install transformers==4.49.0\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8AuhGEqKeqUi"
      },
      "id": "8AuhGEqKeqUi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load SegmentationDatasets from Drive { display-mode: \"code\" }\n",
        "\n",
        "import torch\n",
        "import json\n",
        "\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "torch.serialization.add_safe_globals([SegmentationDataset])\n",
        "\n",
        "# Load the dataset from a binary file\n",
        "def load_dataset(file_path):\n",
        "    dataset = torch.load(file_path)\n",
        "    print(f\"Dataset loaded from {file_path}\")\n",
        "    return dataset\n",
        "\n",
        "# Define file paths for loading\n",
        "train_data_path = '/content/drive/MyDrive/desCartes/pytorch/train_data.pt'\n",
        "eval_data_path = '/content/drive/MyDrive/desCartes/pytorch/eval_data.pt'\n",
        "\n",
        "# Load the datasets from Google Drive\n",
        "eval_dataset = load_dataset(eval_data_path)\n",
        "train_dataset = load_dataset(train_data_path)\n"
      ],
      "metadata": {
        "id": "kLmEZ9AIBh2G"
      },
      "id": "kLmEZ9AIBh2G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Safe Mean Intersection-over-Union { display-mode: \"code\" }\n",
        "\n",
        "# Copyright 2022 The HuggingFace Evaluate Authors.\n",
        "# Based on https://huggingface.co/spaces/evaluate-metric/mean_iou/blob/main/mean_iou.py\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "\n",
        "import evaluate\n",
        "\n",
        "def intersect_and_union(\n",
        "    pred_label,\n",
        "    label,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    if label_map is not None:\n",
        "        for old_id, new_id in label_map.items():\n",
        "            label[label == old_id] = new_id\n",
        "\n",
        "    # turn into Numpy arrays\n",
        "    pred_label = np.array(pred_label)\n",
        "    label = np.array(label)\n",
        "\n",
        "    if reduce_labels:\n",
        "        label[label == 0] = 255\n",
        "        label = label - 1\n",
        "        label[label == 254] = 255\n",
        "\n",
        "    mask = label != ignore_index\n",
        "    mask = np.not_equal(label, ignore_index)\n",
        "    pred_label = pred_label[mask]\n",
        "    label = np.array(label)[mask]\n",
        "\n",
        "    intersect = pred_label[pred_label == label]\n",
        "\n",
        "    area_intersect = np.histogram(intersect, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "    area_pred_label = np.histogram(pred_label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "    area_label = np.histogram(label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "\n",
        "    area_union = area_pred_label + area_label - area_intersect\n",
        "\n",
        "    return area_intersect, area_union, area_pred_label, area_label\n",
        "\n",
        "\n",
        "def total_intersect_and_union(\n",
        "    results,\n",
        "    gt_seg_maps,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    total_area_intersect = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_union = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_pred_label = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_label = np.zeros((num_labels,), dtype=np.float64)\n",
        "    for result, gt_seg_map in zip(results, gt_seg_maps):\n",
        "        area_intersect, area_union, area_pred_label, area_label = intersect_and_union(\n",
        "            result, gt_seg_map, num_labels, ignore_index, label_map, reduce_labels\n",
        "        )\n",
        "        total_area_intersect += area_intersect\n",
        "        total_area_union += area_union\n",
        "        total_area_pred_label += area_pred_label\n",
        "        total_area_label += area_label\n",
        "    return total_area_intersect, total_area_union, total_area_pred_label, total_area_label\n",
        "\n",
        "\n",
        "def mean_iou(\n",
        "    results,\n",
        "    gt_seg_maps,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    nan_to_num: Optional[int] = None,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    total_area_intersect, total_area_union, total_area_pred_label, total_area_label = total_intersect_and_union(\n",
        "        results, gt_seg_maps, num_labels, ignore_index, label_map, reduce_labels\n",
        "    )\n",
        "\n",
        "    # compute metrics\n",
        "    metrics = dict()\n",
        "    eps = 1e-10  # Small constant to prevent division by zero\n",
        "    min_val, max_val = eps, 1 - eps  # Clip range to avoid extreme values\n",
        "    round_decimals = 5  # Number of decimal places for rounding\n",
        "\n",
        "    # Compute metrics with epsilon and clipping\n",
        "    all_acc = np.clip(total_area_intersect.sum() / (total_area_label.sum() + eps), min_val, max_val)\n",
        "    iou = np.clip(total_area_intersect / (total_area_union + eps), min_val, max_val)\n",
        "    acc = np.clip(total_area_intersect / (total_area_label + eps), min_val, max_val)\n",
        "\n",
        "    # Round values, ensuring that values like 9.9999e-01 are rounded up to 1.0\n",
        "    iou = np.round(iou, round_decimals)\n",
        "    acc = np.round(acc, round_decimals)\n",
        "\n",
        "    # Explicitly round values very close to 1.0 (e.g., 0.99999, 0.999999)\n",
        "    iou = np.where(iou >= 0.99999, 1.0, iou)\n",
        "    acc = np.where(acc >= 0.99999, 1.0, acc)\n",
        "\n",
        "    # Assign a default value of 1 for empty classes\n",
        "    non_empty_classes = total_area_label > 0\n",
        "    iou[~non_empty_classes] = 1.0\n",
        "    acc[~non_empty_classes] = 1.0\n",
        "\n",
        "    # Calculate final metrics with rounding\n",
        "    metrics = {\n",
        "        \"mean_iou\": round(np.nanmean(iou), round_decimals),\n",
        "        \"mean_accuracy\": round(np.nanmean(acc), round_decimals),\n",
        "        \"overall_accuracy\": round(all_acc, round_decimals),\n",
        "        \"per_category_iou\": iou,\n",
        "        \"per_category_accuracy\": acc,\n",
        "    }\n",
        "\n",
        "    if nan_to_num is not None:\n",
        "        metrics = dict(\n",
        "            {metric: np.nan_to_num(metric_value, nan=nan_to_num) for metric, metric_value in metrics.items()}\n",
        "        )\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "E4cS_c8TNAup"
      },
      "id": "E4cS_c8TNAup",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train Model { display-mode: \"code\" }\n",
        "\n",
        "if 'mean_iou' not in globals():\n",
        "    raise NameError(\"Function 'mean_iou' is not defined. Run the appropriate cell first.\")\n",
        "\n",
        "if 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
        "    raise NameError(\"Either 'train_dataset' or 'eval_dataset' is not defined. Run the appropriate cell first.\")\n",
        "\n",
        "if not train_dataset:  # Checks if train_dataset is empty\n",
        "    raise ValueError(\"'train_dataset' is empty.\")\n",
        "\n",
        "if not eval_dataset:  # Checks if eval_dataset is empty\n",
        "    raise ValueError(\"'eval_dataset' is empty.\")\n",
        "\n",
        "print(\"All variable checks passed! Proceeding with execution.\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import time\n",
        "import wandb\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.runtime as xr\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from google.colab import userdata\n",
        "\n",
        "# Google Drive Path Configuration\n",
        "project_path = '/content/drive/MyDrive/desCartes'\n",
        "model_path = f'{project_path}/models'\n",
        "results_path = f'{project_path}/results'\n",
        "\n",
        "# Select Model\n",
        "model_version = 'b4'\n",
        "restart_training = False\n",
        "\n",
        "# Define class labels\n",
        "class_labels = [\"background\", \"main_road\", \"minor_road\", \"semi_enclosed_path\", \"unenclosed_path\"]\n",
        "\n",
        "# Local directory for storing dataset\n",
        "local_data_dir = \"/content/data\"\n",
        "\n",
        "# Training Configuration\n",
        "per_device_train_batch_size = 2  # Batch size for training\n",
        "per_device_eval_batch_size = per_device_train_batch_size\n",
        "gradient_accumulation_steps = 1  # Simulates a batch size of gradient_accumulation_steps * per_device_train_batch_size\n",
        "\n",
        "# Loss Function Configuration\n",
        "loss_gamma = 2.0  # Focal loss gamma\n",
        "loss_alpha = 0.25  # Focal loss alpha\n",
        "\n",
        "###################################################\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_TOKEN')\n",
        "!wandb login\n",
        "wandb.init(project=\"tpu-segmentation\", name=f\"TPU-Training-{model_version}\", settings=wandb.Settings(start_method=\"fork\", _service_wait=60))\n",
        "\n",
        "# Configure label mappings\n",
        "num_classes = len(class_labels)\n",
        "id2label = {i: label for i, label in enumerate(class_labels)}\n",
        "label2id = {label: i for i, label in id2label.items()}\n",
        "\n",
        "# Test for existing checkpoints\n",
        "checkpoint_path = f\"{model_path}/checkpoints/{model_version}\"\n",
        "if restart_training and os.path.exists(checkpoint_path):\n",
        "    shutil.rmtree(checkpoint_path)  # Deletes the folder and its contents\n",
        "    os.makedirs(checkpoint_path)  # Recreate the empty checkpoint directory\n",
        "resume_training = os.path.exists(checkpoint_path) and any(os.scandir(checkpoint_path))\n",
        "\n",
        "def load_or_download_segformer():\n",
        "    try:\n",
        "        model_name = f\"nvidia/segformer-{model_version}-finetuned-ade-512-512\"\n",
        "        base_model_path = f'{model_path}/base/{model_name}'\n",
        "\n",
        "        if not os.path.exists(base_model_path):\n",
        "            print(f\"Downloading model from Hugging Face: {model_name}\")\n",
        "            os.makedirs(base_model_path)\n",
        "\n",
        "            hf_token = userdata.get('HF_TOKEN')\n",
        "            if hf_token:\n",
        "                os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "            model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "                model_name,\n",
        "                num_labels=num_classes,\n",
        "                id2label=id2label,\n",
        "                label2id=label2id,\n",
        "                ignore_mismatched_sizes=True,\n",
        "            )\n",
        "            model.save_pretrained(base_model_path)\n",
        "        else:\n",
        "            model = SegformerForSemanticSegmentation.from_pretrained(base_model_path)\n",
        "\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading/downloading: {e}\")\n",
        "        raise\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            logits, labels = eval_pred\n",
        "            logits_tensor = torch.from_numpy(logits)\n",
        "\n",
        "            # Upsample logits to match labels\n",
        "            logits_tensor = nn.functional.interpolate(\n",
        "                logits_tensor,\n",
        "                size=labels.shape[-2:],  # Match height & width of labels\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            ).argmax(dim=1)  # Convert to predicted class indices\n",
        "\n",
        "            pred_labels = logits_tensor.detach().cpu().numpy()\n",
        "\n",
        "            # Call the safe mean_iou function (defined in another cell)\n",
        "            metrics = mean_iou(\n",
        "                results=pred_labels,\n",
        "                gt_seg_maps=labels,\n",
        "                num_labels=num_classes,\n",
        "                ignore_index=True,\n",
        "                reduce_labels=False,\n",
        "            )\n",
        "\n",
        "            # Extract per-class IoU & accuracy\n",
        "            per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
        "            per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
        "\n",
        "            # Compute precision, recall, and F1-score (excluding background)\n",
        "            pred_flat = pred_labels.flatten()\n",
        "            labels_flat = labels.flatten()\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                labels_flat, pred_flat, average=\"weighted\", zero_division=0\n",
        "            )\n",
        "\n",
        "            # Store overall metrics\n",
        "            metrics[\"overall_accuracy\"] = metrics.pop(\"mean_accuracy\")\n",
        "            metrics[\"overall_mean_iou\"] = metrics.pop(\"mean_iou\")\n",
        "            metrics[\"precision\"] = precision\n",
        "            metrics[\"recall\"] = recall\n",
        "            metrics[\"f1_score\"] = f1\n",
        "\n",
        "            # Add per-class accuracy & IoU\n",
        "            metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
        "            metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return default zeroed metrics with an error flag\n",
        "        return {\n",
        "            \"error\": True,\n",
        "            \"overall_accuracy\": 0.0,\n",
        "            \"overall_mean_iou\": 0.0,\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"f1_score\": 0.0,\n",
        "            **{f\"accuracy_{id2label[i]}\": 0.0 for i in range(num_classes)},\n",
        "            **{f\"iou_{id2label[i]}\": 0.0 for i in range(num_classes)},\n",
        "        }\n",
        "\n",
        "def tpu_worker_process(rank):\n",
        "\n",
        "    try:\n",
        "        # Set TPU device\n",
        "        device = xm.xla_device()\n",
        "        world_size = xr.world_size()\n",
        "\n",
        "        # Ensure that all TPUs are available before proceeding\n",
        "        xm.rendezvous(\"ready\")\n",
        "        xm.master_print(f\"All {world_size} devices are ready!\\n\", flush=True)\n",
        "\n",
        "        # Ensure that model is not fetched from HF more than once\n",
        "        # Note: model cannot successfully be passed into this function and mounted on each device\n",
        "        if rank == 0:\n",
        "            model = load_or_download_segformer()\n",
        "            model.to(device)\n",
        "            xm.rendezvous(\"model_ready\")\n",
        "        else:\n",
        "            xm.rendezvous(\"model_ready\")\n",
        "            model = load_or_download_segformer()\n",
        "            model.to(device)\n",
        "\n",
        "        # Distributed samplers (drop_last=True to prevent hanging)\n",
        "        train_sampler = DistributedSampler(\n",
        "            train_dataset, num_replicas=world_size, rank=rank, shuffle=True, drop_last=True\n",
        "        )\n",
        "        eval_sampler = DistributedSampler(\n",
        "            eval_dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=True\n",
        "        )\n",
        "\n",
        "        # Safe TPU DataLoader setup\n",
        "        def worker_init_fn(worker_id):\n",
        "            \"\"\"Ensures each worker has a different random seed\"\"\"\n",
        "            torch.manual_seed(worker_id + rank)\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset, batch_size=per_device_train_batch_size, sampler=train_sampler,\n",
        "            num_workers=4, pin_memory=True, persistent_workers=True, worker_init_fn=worker_init_fn\n",
        "        )\n",
        "        eval_dataloader = DataLoader(\n",
        "            eval_dataset, batch_size=per_device_eval_batch_size, sampler=eval_sampler,\n",
        "            num_workers=4, pin_memory=True, persistent_workers=True, worker_init_fn=worker_init_fn\n",
        "        )\n",
        "\n",
        "        # Wrap data loaders with MpDeviceLoader for TPU support\n",
        "        train_dataloader = pl.MpDeviceLoader(train_dataloader, device)\n",
        "        eval_dataloader = pl.MpDeviceLoader(eval_dataloader, device)\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=checkpoint_path,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "            dataloader_num_workers=4,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=10,\n",
        "            logging_strategy=\"steps\",\n",
        "            report_to=[\"wandb\"] if rank == 0 else [],\n",
        "            disable_tqdm=(rank != 0),\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            fp16=False,\n",
        "            bf16=True,\n",
        "            metric_for_best_model=\"overall_mean_iou\",  # Metric to monitor for best model\n",
        "            greater_is_better=True,  # Set to True to maximize the metric\n",
        "            num_train_epochs=100,\n",
        "            save_total_limit=5,  # Keep only the last 5 checkpoints\n",
        "            load_best_model_at_end=True,\n",
        "            push_to_hub=False,\n",
        "            run_name=f\"desCartes-{model_version}-{per_device_train_batch_size}-{gradient_accumulation_steps}-bf16\"\n",
        "        )\n",
        "\n",
        "        # Ensure that all TPUs are properly loaded before proceeding\n",
        "        xm.rendezvous(\"steady\")\n",
        "        xm.master_print(\"All devices are steady!\\n\", flush=True)\n",
        "\n",
        "        # Trainer: override standard methods\n",
        "        class CustomTrainer(Trainer):\n",
        "            def get_train_dataloader(self):\n",
        "                return train_dataloader\n",
        "\n",
        "            def get_eval_dataloader(self, eval_dataset=None):\n",
        "                return eval_dataloader\n",
        "\n",
        "            # TODO: The following function needs to be fixed to optimise class-imbalance training\n",
        "            # At present, even returning a \"safe\" value causes TPU-reporting to drop out after less than 1 minute\n",
        "\n",
        "            # def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, gamma=loss_gamma, alpha=loss_alpha):\n",
        "            #     \"\"\"\n",
        "            #     Custom loss computation for Hugging Face Trainer, preserving compatibility with the original logic.\n",
        "            #     \"\"\"\n",
        "\n",
        "            #     labels = inputs.pop(\"labels\", None)  # Extract labels if present\n",
        "\n",
        "            #     if self.model_accepts_loss_kwargs:\n",
        "            #         loss_kwargs = {}\n",
        "            #         if num_items_in_batch is not None:\n",
        "            #             loss_kwargs[\"num_items_in_batch\"] = num_items_in_batch\n",
        "            #         inputs = {**inputs, **loss_kwargs}\n",
        "\n",
        "            #     outputs = model(**inputs)\n",
        "\n",
        "            #     if self.args.past_index >= 0:\n",
        "            #         self._past = outputs[self.args.past_index]\n",
        "\n",
        "            #     if labels is not None:\n",
        "\n",
        "            #         loss = torch.tensor(0.0, device=labels.device) # <<< BUGFIXING: Return safe value?\n",
        "\n",
        "            #         # logits = outputs.logits\n",
        "\n",
        "            #         # # Ensure device consistency\n",
        "            #         # if logits.device != labels.device:\n",
        "            #         #     labels = labels.to(logits.device)\n",
        "\n",
        "            #         # # Compute cross-entropy loss\n",
        "            #         # ce_loss = F.cross_entropy(logits, labels, reduction=\"none\").float()\n",
        "\n",
        "            #         # # Compute focal loss components\n",
        "            #         # pt = torch.exp(-torch.clamp(ce_loss, min=1e-8, max=1e8))  # Avoid instability\n",
        "            #         # focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
        "\n",
        "            #         # loss = focal_loss.mean()\n",
        "\n",
        "            #     else:\n",
        "            #         # Handle case where model should return loss directly\n",
        "            #         if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
        "            #             raise ValueError(\n",
        "            #                 \"The model did not return a loss from the inputs, only the following keys: \"\n",
        "            #                 f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
        "            #             )\n",
        "            #         loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "\n",
        "            #     # Adjust loss scaling if using multi-device\n",
        "            #     if self.args.average_tokens_across_devices and self.model_accepts_loss_kwargs:\n",
        "            #         loss *= self.accelerator.num_processes\n",
        "\n",
        "            #     return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "        trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "        )\n",
        "\n",
        "        # Synchronize TPUs before starting training\n",
        "        xm.rendezvous(\"start_training\")  # Ensure all TPU processes sync before proceeding\n",
        "        xm.master_print(f\"All devices are GO! ... training started (resume={resume_training})...\\n\", flush=True)\n",
        "\n",
        "        trainer.train(resume_from_checkpoint=resume_training)\n",
        "        xm.rendezvous(\"training_complete\")  # Ensure all TPU processes sync before exit\n",
        "        xm.master_print(\"Training completed!\\n\", flush=True)\n",
        "\n",
        "        # Terminate WandB logging\n",
        "        if rank == 0:\n",
        "            wandb.finish()\n",
        "\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "    return\n",
        "\n",
        "# Launch TPU training with WandB logging\n",
        "xmp.spawn(tpu_worker_process, args=(), start_method='fork')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "b8zQKwQSfj8k",
        "outputId": "1806e1ee-ad45-4563-b21c-848e29f535ac"
      },
      "id": "b8zQKwQSfj8k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All variable checks passed! Proceeding with execution.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdocuracy\u001b[0m (\u001b[33mdocuracy-university-of-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdocuracy\u001b[0m (\u001b[33mdocuracy-university-of-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_232624-0b94sx7m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation/runs/0b94sx7m' target=\"_blank\">TPU-Training-b4</a></strong> to <a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation' target=\"_blank\">https://wandb.ai/docuracy-university-of-london/tpu-segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation/runs/0b94sx7m' target=\"_blank\">https://wandb.ai/docuracy-university-of-london/tpu-segmentation/runs/0b94sx7m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 8 devices are ready!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices are steady!\n",
            "\n",
            "All devices are GO! ... training started (resume=False)...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='653' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  653/10800 37:57 < 9:51:33, 0.29 it/s, Epoch 6.04/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.611600</td>\n",
              "      <td>0.507012</td>\n",
              "      <td>0.399550</td>\n",
              "      <td>0.390820</td>\n",
              "      <td>0.901888</td>\n",
              "      <td>0.946136</td>\n",
              "      <td>0.922573</td>\n",
              "      <td>0.997460</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.953800</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.337616</td>\n",
              "      <td>0.399980</td>\n",
              "      <td>0.391190</td>\n",
              "      <td>0.899766</td>\n",
              "      <td>0.948170</td>\n",
              "      <td>0.923047</td>\n",
              "      <td>0.999910</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.955950</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.248300</td>\n",
              "      <td>0.259972</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.391200</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.956020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.244000</td>\n",
              "      <td>0.254836</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.391200</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.956020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.250700</td>\n",
              "      <td>0.253241</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.391200</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.956020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.236600</td>\n",
              "      <td>0.251325</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.391200</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.956020</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}