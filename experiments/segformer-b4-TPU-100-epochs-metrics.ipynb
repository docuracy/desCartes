{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docuracy/desCartes/blob/main/experiments/segformer-b4-TPU-100-epochs-metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Authenticate WandB and Hugging Face; mount Google Drive; install dependencies\n",
        "import os\n",
        "\n",
        "!pip install wandb -qU\n",
        "!wandb login\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "\n",
        "!pip install opencv-python\n",
        "!pip install --upgrade torch_xla torch\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xx4m0dgRcKNe"
      },
      "id": "Xx4m0dgRcKNe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load SegmentationDatasets from Drive { display-mode: \"code\" }\n",
        "\n",
        "import torch\n",
        "\n",
        "class SegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "torch.serialization.add_safe_globals([SegmentationDataset])\n",
        "\n",
        "# Load the dataset from a binary file\n",
        "def load_dataset(file_path):\n",
        "    dataset = torch.load(file_path)\n",
        "    print(f\"Dataset loaded from {file_path}\")\n",
        "    return dataset\n",
        "\n",
        "# Define file paths for loading\n",
        "train_data_path = '/content/drive/MyDrive/desCartes/pytorch/train_data.pt'\n",
        "eval_data_path = '/content/drive/MyDrive/desCartes/pytorch/eval_data.pt'\n",
        "\n",
        "# Load the datasets from Google Drive\n",
        "eval_dataset = load_dataset(eval_data_path)\n",
        "train_dataset = load_dataset(train_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLmEZ9AIBh2G",
        "outputId": "3d5597d8-1887-435d-dd38-d9955c9d7fa1"
      },
      "id": "kLmEZ9AIBh2G",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded from /content/drive/MyDrive/desCartes/pytorch/eval_data.pt\n",
            "Dataset loaded from /content/drive/MyDrive/desCartes/pytorch/train_data.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Safe Mean Intersection-over-Union { display-mode: \"code\" }\n",
        "\n",
        "# Copyright 2022 The HuggingFace Evaluate Authors.\n",
        "# Based on https://huggingface.co/spaces/evaluate-metric/mean_iou/blob/main/mean_iou.py\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "\n",
        "import evaluate\n",
        "\n",
        "def intersect_and_union(\n",
        "    pred_label,\n",
        "    label,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    if label_map is not None:\n",
        "        for old_id, new_id in label_map.items():\n",
        "            label[label == old_id] = new_id\n",
        "\n",
        "    # turn into Numpy arrays\n",
        "    pred_label = np.array(pred_label)\n",
        "    label = np.array(label)\n",
        "\n",
        "    if reduce_labels:\n",
        "        label[label == 0] = 255\n",
        "        label = label - 1\n",
        "        label[label == 254] = 255\n",
        "\n",
        "    mask = label != ignore_index\n",
        "    mask = np.not_equal(label, ignore_index)\n",
        "    pred_label = pred_label[mask]\n",
        "    label = np.array(label)[mask]\n",
        "\n",
        "    intersect = pred_label[pred_label == label]\n",
        "\n",
        "    area_intersect = np.histogram(intersect, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "    area_pred_label = np.histogram(pred_label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "    area_label = np.histogram(label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
        "\n",
        "    area_union = area_pred_label + area_label - area_intersect\n",
        "\n",
        "    return area_intersect, area_union, area_pred_label, area_label\n",
        "\n",
        "\n",
        "def total_intersect_and_union(\n",
        "    results,\n",
        "    gt_seg_maps,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    total_area_intersect = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_union = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_pred_label = np.zeros((num_labels,), dtype=np.float64)\n",
        "    total_area_label = np.zeros((num_labels,), dtype=np.float64)\n",
        "    for result, gt_seg_map in zip(results, gt_seg_maps):\n",
        "        area_intersect, area_union, area_pred_label, area_label = intersect_and_union(\n",
        "            result, gt_seg_map, num_labels, ignore_index, label_map, reduce_labels\n",
        "        )\n",
        "        total_area_intersect += area_intersect\n",
        "        total_area_union += area_union\n",
        "        total_area_pred_label += area_pred_label\n",
        "        total_area_label += area_label\n",
        "    return total_area_intersect, total_area_union, total_area_pred_label, total_area_label\n",
        "\n",
        "\n",
        "def mean_iou(\n",
        "    results,\n",
        "    gt_seg_maps,\n",
        "    num_labels,\n",
        "    ignore_index: bool,\n",
        "    nan_to_num: Optional[int] = None,\n",
        "    label_map: Optional[Dict[int, int]] = None,\n",
        "    reduce_labels: bool = False,\n",
        "):\n",
        "    total_area_intersect, total_area_union, total_area_pred_label, total_area_label = total_intersect_and_union(\n",
        "        results, gt_seg_maps, num_labels, ignore_index, label_map, reduce_labels\n",
        "    )\n",
        "\n",
        "    # compute metrics\n",
        "    metrics = dict()\n",
        "    eps = 1e-10  # Small constant to prevent division by zero\n",
        "    min_val, max_val = 1e-5, 1 - 1e-5  # Clip range to avoid extreme values\n",
        "    round_decimals = 5  # Number of decimal places for rounding\n",
        "\n",
        "    # Compute metrics with epsilon and clipping\n",
        "    all_acc = np.clip(total_area_intersect.sum() / (total_area_label.sum() + eps), min_val, max_val)\n",
        "    iou = np.clip(total_area_intersect / (total_area_union + eps), min_val, max_val)\n",
        "    acc = np.clip(total_area_intersect / (total_area_label + eps), min_val, max_val)\n",
        "\n",
        "    # Round values, ensuring that values like 9.9999e-01 are rounded up to 1.0\n",
        "    iou = np.round(iou, round_decimals)\n",
        "    acc = np.round(acc, round_decimals)\n",
        "\n",
        "    # Explicitly round values very close to 1.0 (e.g., 0.99999, 0.999999)\n",
        "    iou = np.where(iou >= 0.99999, 1.0, iou)\n",
        "    acc = np.where(acc >= 0.99999, 1.0, acc)\n",
        "\n",
        "    # Calculate final metrics with rounding\n",
        "    metrics = {\n",
        "        \"mean_iou\": round(np.nanmean(iou), round_decimals),\n",
        "        \"mean_accuracy\": round(np.nanmean(acc), round_decimals),\n",
        "        \"overall_accuracy\": round(all_acc, round_decimals),\n",
        "        \"per_category_iou\": iou,\n",
        "        \"per_category_accuracy\": acc,\n",
        "    }\n",
        "\n",
        "    if nan_to_num is not None:\n",
        "        metrics = dict(\n",
        "            {metric: np.nan_to_num(metric_value, nan=nan_to_num) for metric, metric_value in metrics.items()}\n",
        "        )\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "E4cS_c8TNAup"
      },
      "id": "E4cS_c8TNAup",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LAVtAKuebADW",
        "outputId": "1ba1a726-75c8-46e8-d38c-676c6a94af26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All variable checks passed! Proceeding with execution.\n",
            "Process 0/8 using device xla:0\n",
            "Initialising WandB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdocuracy\u001b[0m (\u001b[33mdocuracy-university-of-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250325_163002-lxpppjhe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation/runs/lxpppjhe' target=\"_blank\">TPU-Training-b4</a></strong> to <a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation' target=\"_blank\">https://wandb.ai/docuracy-university-of-london/tpu-segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/docuracy-university-of-london/tpu-segmentation/runs/lxpppjhe' target=\"_blank\">https://wandb.ai/docuracy-university-of-london/tpu-segmentation/runs/lxpppjhe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([5, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPUs synchronised. Starting training...TPUs synchronised. Starting training...TPUs synchronised. Starting training...TPUs synchronised. Starting training...TPUs synchronised. Starting training...TPUs synchronised. Starting training...TPUs synchronised. Starting training...\n",
            "TPUs synchronised. Starting training...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:57 < 18:58:33, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:56 < 18:58:06, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:54 < 18:54:29, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:54 < 18:54:22, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:54 < 18:54:26, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:54 < 18:54:31, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:54 < 18:54:19, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='142' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  142/10800 14:53 < 18:54:15, 0.16 it/s, Epoch 1.31/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Overall Accuracy</th>\n",
              "      <th>Overall Mean Iou</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Accuracy Background</th>\n",
              "      <th>Accuracy Main Road</th>\n",
              "      <th>Accuracy Minor Road</th>\n",
              "      <th>Accuracy Semi Enclosed Path</th>\n",
              "      <th>Accuracy Unenclosed Path</th>\n",
              "      <th>Iou Background</th>\n",
              "      <th>Iou Main Road</th>\n",
              "      <th>Iou Minor Road</th>\n",
              "      <th>Iou Semi Enclosed Path</th>\n",
              "      <th>Iou Unenclosed Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.673900</td>\n",
              "      <td>0.354130</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.899180</td>\n",
              "      <td>0.948251</td>\n",
              "      <td>0.923064</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Train Model { display-mode: \"code\" }\n",
        "\n",
        "if 'mean_iou' not in globals():\n",
        "    raise NameError(\"Function 'mean_iou' is not defined. Run the appropriate cell first.\")\n",
        "\n",
        "if 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
        "    raise NameError(\"Either 'train_dataset' or 'eval_dataset' is not defined. Run the appropriate cell first.\")\n",
        "\n",
        "if not train_dataset:  # Checks if train_dataset is empty\n",
        "    raise ValueError(\"'train_dataset' is empty.\")\n",
        "\n",
        "if not eval_dataset:  # Checks if eval_dataset is empty\n",
        "    raise ValueError(\"'eval_dataset' is empty.\")\n",
        "\n",
        "print(\"All variable checks passed! Proceeding with execution.\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import wandb\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.runtime as xr\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "# Tidy up output [ineffective for TPUs]\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", message=\"Some weights of SegformerForSemanticSegmentation were not initialized\", category=UserWarning)\n",
        "# warnings.filterwarnings(\"ignore\", message=\".*feature_extractor_type.*\", category=UserWarning)\n",
        "\n",
        "# Google Drive Path Configuration\n",
        "project_path = '/content/drive/MyDrive/desCartes'\n",
        "model_path = f'{project_path}/models'\n",
        "results_path = f'{project_path}/results'\n",
        "\n",
        "# Select Model\n",
        "model_version = 'b4'\n",
        "\n",
        "# Define class labels\n",
        "class_labels = [\"background\", \"main_road\", \"minor_road\", \"semi_enclosed_path\", \"unenclosed_path\"]\n",
        "\n",
        "# Local directory for storing dataset\n",
        "local_data_dir = \"/content/data\"\n",
        "\n",
        "# Training Configuration\n",
        "per_device_train_batch_size = 2  # Batch size for training\n",
        "per_device_eval_batch_size = per_device_train_batch_size\n",
        "gradient_accumulation_steps = 1  # Simulates a batch size of gradient_accumulation_steps * per_device_train_batch_size\n",
        "\n",
        "###################################################\n",
        "\n",
        "# Configure label mappings\n",
        "num_classes = len(class_labels)\n",
        "id2label = {i: label for i, label in enumerate(class_labels)}\n",
        "label2id = {label: i for i, label in id2label.items()}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            logits, labels = eval_pred\n",
        "            logits_tensor = torch.from_numpy(logits)\n",
        "\n",
        "            # Upsample logits to match labels\n",
        "            logits_tensor = nn.functional.interpolate(\n",
        "                logits_tensor,\n",
        "                size=labels.shape[-2:],  # Match height & width of labels\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            ).argmax(dim=1)  # Convert to predicted class indices\n",
        "\n",
        "            pred_labels = logits_tensor.detach().cpu().numpy()\n",
        "\n",
        "            # Call the safe mean_iou function (defined in another cell)\n",
        "            metrics = mean_iou(\n",
        "                results=pred_labels,\n",
        "                gt_seg_maps=labels,\n",
        "                num_labels=num_classes,\n",
        "                ignore_index=0, # class 0 is background/ignored\n",
        "                reduce_labels=False,\n",
        "            )\n",
        "\n",
        "            # Extract per-class IoU & accuracy\n",
        "            per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
        "            per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
        "\n",
        "            # Compute precision, recall, and F1-score (excluding background)\n",
        "            pred_flat = pred_labels.flatten()\n",
        "            labels_flat = labels.flatten()\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                labels_flat, pred_flat, average=\"weighted\", zero_division=0\n",
        "            )\n",
        "\n",
        "            # Store overall metrics\n",
        "            metrics[\"overall_accuracy\"] = metrics.pop(\"mean_accuracy\")\n",
        "            metrics[\"overall_mean_iou\"] = metrics.pop(\"mean_iou\")\n",
        "            metrics[\"precision\"] = precision\n",
        "            metrics[\"recall\"] = recall\n",
        "            metrics[\"f1_score\"] = f1\n",
        "\n",
        "            # Add per-class accuracy & IoU\n",
        "            metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
        "            metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return default zeroed metrics with an error flag\n",
        "        return {\n",
        "            \"error\": True,\n",
        "            \"overall_accuracy\": 0.0,\n",
        "            \"overall_mean_iou\": 0.0,\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"f1_score\": 0.0,\n",
        "            **{f\"accuracy_{id2label[i]}\": 0.0 for i in range(num_classes)},\n",
        "            **{f\"iou_{id2label[i]}\": 0.0 for i in range(num_classes)},\n",
        "        }\n",
        "\n",
        "def _mp_fn(rank, hf_token):\n",
        "    # Set TPU device inside the function\n",
        "    device = xm.xla_device()\n",
        "    world_size = xr.world_size()\n",
        "    xm.master_print(f\"Process {rank}/{world_size} using device {device}\")\n",
        "\n",
        "    # Initialise WandB only for the main TPU process\n",
        "    if rank == 0:\n",
        "        print(\"Initialising WandB...\")\n",
        "        wandb.init(project=\"tpu-segmentation\", name=f\"TPU-Training-{model_version}\")\n",
        "\n",
        "    # Load the image processor and model inside _mp_fn\n",
        "    os.environ['HF_TOKEN'] = hf_token\n",
        "    image_processor = SegformerImageProcessor.from_pretrained(f'nvidia/segformer-{model_version}-finetuned-ade-512-512')\n",
        "\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        f\"nvidia/segformer-{model_version}-finetuned-ade-512-512\",\n",
        "        num_labels=num_classes,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    ).to(device)\n",
        "\n",
        "    # Distributed samplers (drop_last=True to prevent hanging)\n",
        "    train_sampler = DistributedSampler(\n",
        "        train_dataset, num_replicas=world_size, rank=rank, shuffle=True, drop_last=True\n",
        "    )\n",
        "    eval_sampler = DistributedSampler(\n",
        "        eval_dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    # Safe TPU DataLoader setup\n",
        "    def worker_init_fn(worker_id):\n",
        "        \"\"\"Ensures each worker has a different random seed\"\"\"\n",
        "        torch.manual_seed(worker_id + rank)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, batch_size=per_device_train_batch_size, sampler=train_sampler,\n",
        "        num_workers=4, pin_memory=True, persistent_workers=True, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, batch_size=per_device_eval_batch_size, sampler=eval_sampler,\n",
        "        num_workers=4, pin_memory=True, persistent_workers=True, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    # Wrap data loaders with MpDeviceLoader for TPU support\n",
        "    train_dataloader = pl.MpDeviceLoader(train_dataloader, device)\n",
        "    eval_dataloader = pl.MpDeviceLoader(eval_dataloader, device)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"{model_path}/checkpoints\",\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        dataloader_num_workers=4,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=5,  # Keep only the last 5 checkpoints\n",
        "        logging_steps=10,\n",
        "        logging_strategy=\"steps\",\n",
        "        report_to=[\"wandb\"] if rank == 0 else [],\n",
        "        disable_tqdm=(rank != 0),\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        num_train_epochs=100,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        metric_for_best_model=\"overall_mean_iou\",  # Metric to monitor for best model\n",
        "        greater_is_better=True,  # Set to True to maximize the metric\n",
        "        run_name=f\"desCartes-{model_version}-{per_device_train_batch_size}-{gradient_accumulation_steps}-bf16\"\n",
        "    )\n",
        "\n",
        "    # Trainer: override standard dataloader methods\n",
        "    class CustomTrainer(Trainer):\n",
        "        def get_train_dataloader(self):\n",
        "            return train_dataloader\n",
        "\n",
        "        def get_eval_dataloader(self, eval_dataset=None):\n",
        "            return eval_dataloader\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "    )\n",
        "\n",
        "    # Synchronize TPUs before starting\n",
        "    xm.rendezvous(\"start_training\")  # Ensure all TPU processes sync before proceeding\n",
        "    if rank == 0:\n",
        "        print(\"TPUs synchronised. Starting training...\")\n",
        "\n",
        "    # trainer.train(resume_from_checkpoint=os.path.exists(f\"{model_path}/checkpoints\"))\n",
        "    trainer.train(resume_from_checkpoint=False)\n",
        "    xm.rendezvous(\"training_complete\")  # Ensure all TPU processes sync before exit\n",
        "\n",
        "    if rank == 0:\n",
        "        wandb.finish()  # Close WandB properly [Leave open for metrics via API]\n",
        "\n",
        "\n",
        "# Launch TPU training\n",
        "if __name__ == \"__main__\":\n",
        "    hf_token = os.getenv('HF_TOKEN')\n",
        "    xmp.spawn(_mp_fn, args=(hf_token,), start_method='fork')\n"
      ],
      "id": "LAVtAKuebADW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}